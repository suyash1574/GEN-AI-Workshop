{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfXOPoz4cXoI"
      },
      "source": [
        "# Day 1: Environment Check & Setup\n",
        "\n",
        "**Duration:** 30 minutes  \n",
        "**Objective:** Verify your development environment and understand the workshop toolkit\n",
        "\n",
        "## üéØ Learning Goals\n",
        "By the end of this notebook, you will:\n",
        "- Understand how to verify Python package installations\n",
        "- Know how to check GPU availability for deep learning\n",
        "- Be familiar with basic PyTorch tensor operations\n",
        "- Understand the importance of environment consistency in AI projects\n",
        "\n",
        "## üìö Key Concepts\n",
        "- **Virtual Environments**: Isolated Python environments for project dependencies\n",
        "- **GPU Computing**: Using graphics cards to accelerate machine learning computations\n",
        "- **Package Management**: Managing Python libraries and their versions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgE5T7nacXoM"
      },
      "source": [
        "## Section 1: Python Environment Verification\n",
        "\n",
        "### üîç What We're Doing\n",
        "First, we need to verify that your Python environment is correctly set up. This includes checking:\n",
        "- Python version (should be 3.10+)\n",
        "- Virtual environment activation\n",
        "- Package availability\n",
        "\n",
        "### üí° Why This Matters\n",
        "Environment consistency is crucial in AI/ML projects. Different Python or package versions can lead to:\n",
        "- Code that works on one machine but fails on another\n",
        "- Subtle differences in model behavior\n",
        "- Incompatible dependencies\n",
        "\n",
        "### üìù Your Task\n",
        "Complete the code below to check your Python version and print the executable path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vueBW448cXoM",
        "outputId": "6997fab6-c09b-41bb-c2a3-10bacdf9febf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Python executable: /usr/bin/python3\n",
            "‚úÖ Python version is compatible!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Import the sys module\n",
        "import sys\n",
        "\n",
        "# TODO: Print the Python version using sys.version\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# TODO: Print the Python executable path using sys.executable\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "\n",
        "# TODO: Check if Python version is 3.10 or higher\n",
        "version_info = sys.version_info\n",
        "if version_info.major == 3 and version_info.minor >= 10:\n",
        "    print(\"‚úÖ Python version is compatible!\")\n",
        "else:\n",
        "    print(\"‚ùå Python version is too old. Please upgrade to 3.10+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThPQbNCIcXoN"
      },
      "source": [
        "### üîß Package Import Verification\n",
        "\n",
        "Now let's verify that all required packages are installed. If any imports fail, you'll need to install them using `pip install -r requirements.txt`.\n",
        "\n",
        "**Research Task:** Look up what each of these packages does:\n",
        "- `torch`: ?\n",
        "- `transformers`: ?\n",
        "- `cv2` (opencv): ?\n",
        "- `gradio`: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7SA_XlcXoO",
        "outputId": "8cc6e2a9-1373-40ba-fafa-0079794cc2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ torch - PyTorch deep learning framework\n",
            "‚úÖ transformers - Hugging Face transformers library\n",
            "‚úÖ cv2 - OpenCV computer vision library\n",
            "‚úÖ numpy - Numerical computing library\n",
            "‚úÖ pandas - Data manipulation library\n",
            "‚úÖ matplotlib - Plotting library\n",
            "‚úÖ gradio - Web UI library for ML demos\n",
            "\n",
            "üéâ All packages imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Try importing all required packages and handle any ImportError\n",
        "required_packages = {\n",
        "    'torch': 'PyTorch deep learning framework',\n",
        "    'transformers': 'Hugging Face transformers library',\n",
        "    'cv2': 'OpenCV computer vision library',\n",
        "    'numpy': 'Numerical computing library',\n",
        "    'pandas': 'Data manipulation library',\n",
        "    'matplotlib': 'Plotting library',\n",
        "    'gradio': 'Web UI library for ML demos'\n",
        "}\n",
        "\n",
        "import_results = []\n",
        "\n",
        "for package_name, description in required_packages.items():\n",
        "    try:\n",
        "        # TODO: Use __import__() or importlib to import the package\n",
        "        __import__(package_name)\n",
        "\n",
        "        print(f\"‚úÖ {package_name} - {description}\")\n",
        "        import_results.append((package_name, True))\n",
        "    except ImportError as e:\n",
        "        # TODO: Handle the import error and print a helpful message\n",
        "        print(f\"‚ùå {package_name} - Missing! {description}. Error: {e}\")\n",
        "        import_results.append((package_name, False))\n",
        "\n",
        "# TODO: Print summary of results\n",
        "failed_imports = [pkg for pkg, success in import_results if not success]\n",
        "if failed_imports:\n",
        "    print(f\"\\n‚ö†Ô∏è Install missing packages: pip install {' '.join(failed_imports)}\")\n",
        "else:\n",
        "    print(\"\\nüéâ All packages imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCOVz0bcXoO"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: GPU and Hardware Check\n",
        "\n",
        "### üîç What We're Doing\n",
        "We'll check if your system has GPU support available for accelerated computing.\n",
        "\n",
        "### üí° Why This Matters\n",
        "- **CPU vs GPU**: CPUs are great for general computing, GPUs excel at parallel operations\n",
        "- **Deep Learning**: Neural networks involve massive matrix operations that GPUs can accelerate 10-100x\n",
        "- **CUDA**: NVIDIA's parallel computing platform that PyTorch uses\n",
        "\n",
        "### üìù Your Task\n",
        "Research and implement GPU detection code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPSMPWD2cXoP",
        "outputId": "983c9f63-7c6f-4ca1-b146-1b515c3655f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n",
            "‚ö†Ô∏è No GPU available - using CPU\n",
            "üí° Consider using Google Colab for free GPU access\n",
            "\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Check if CUDA is available\n",
        "cuda_available = torch.cuda.is_available()  # Use torch.cuda method\n",
        "\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "\n",
        "if cuda_available:\n",
        "    # TODO: Get GPU device name\n",
        "    gpu_name = torch.cuda.get_device_name(0)  # Use torch.cuda method\n",
        "    print(f\"GPU device: {gpu_name}\")\n",
        "\n",
        "    # TODO: Get CUDA version\n",
        "    cuda_version = torch.version.cuda  # Use torch.version attribute\n",
        "    print(f\"CUDA version: {cuda_version}\")\n",
        "\n",
        "    # TODO: Get number of available GPUs\n",
        "    gpu_count = torch.cuda.device_count()  # Use torch.cuda method\n",
        "    print(f\"Number of GPUs: {gpu_count}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available - using CPU\")\n",
        "    print(\"üí° Consider using Google Colab for free GPU access\")\n",
        "\n",
        "# TODO: Set device for computations\n",
        "device = torch.device('cuda' if cuda_available else 'cpu')  # torch.device\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llG6jrROcXoP"
      },
      "source": [
        "### üß™ Memory and Performance Check\n",
        "\n",
        "Let's check available memory and run a simple performance test.\n",
        "\n",
        "**Research Questions:**\n",
        "1. What's the difference between RAM and VRAM?\n",
        "2. Why might you run out of GPU memory during training?\n",
        "3. What strategies can you use if you have limited GPU memory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut6BwGxPcXoQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import psutil  # You may need to install this: pip install psutil\n",
        "\n",
        "# System RAM check\n",
        "ram_info = psutil.virtual_memory()\n",
        "print(f\"Total RAM: {ram_info.total / (1024**3):.1f} GB\")\n",
        "print(f\"Available RAM: {ram_info.available / (1024**3):.1f} GB\")\n",
        "\n",
        "# GPU memory check (if available)\n",
        "if cuda_available:\n",
        "    # TODO: Get GPU memory info using torch.cuda methods\n",
        "    total_memory = ___  # torch.cuda.get_device_properties(0).total_memory\n",
        "    allocated_memory = ___  # torch.cuda.memory_allocated(0)\n",
        "\n",
        "    print(f\"\\nGPU Total Memory: {total_memory / (1024**3):.1f} GB\")\n",
        "    print(f\"GPU Allocated Memory: {allocated_memory / (1024**3):.1f} GB\")\n",
        "\n",
        "# Simple performance test\n",
        "print(\"\\nüî¨ Running performance test...\")\n",
        "\n",
        "# TODO: Create two large random tensors\n",
        "size = 1000\n",
        "a = torch.randn(size, size)  # Create on CPU first\n",
        "b = torch.randn(size, size)\n",
        "\n",
        "# CPU timing\n",
        "start_time = time.time()\n",
        "# TODO: Perform matrix multiplication on CPU\n",
        "result_cpu = ___  # torch.matmul or @ operator\n",
        "cpu_time = time.time() - start_time\n",
        "\n",
        "print(f\"CPU computation time: {cpu_time:.4f} seconds\")\n",
        "\n",
        "# GPU timing (if available)\n",
        "if cuda_available:\n",
        "    # TODO: Move tensors to GPU\n",
        "    a_gpu = ___  # tensor.to(device) or tensor.cuda()\n",
        "    b_gpu = ___\n",
        "\n",
        "    # TODO: Warm up GPU (run operation once)\n",
        "    _ = torch.matmul(a_gpu, b_gpu)\n",
        "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "\n",
        "    start_time = time.time()\n",
        "    # TODO: Perform matrix multiplication on GPU\n",
        "    result_gpu = ___\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "\n",
        "    print(f\"GPU computation time: {gpu_time:.4f} seconds\")\n",
        "    print(f\"Speedup: {cpu_time/gpu_time:.1f}x faster on GPU\")\n",
        "\n",
        "print(\"‚úÖ Performance test complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZN9GtwecXoR"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: Basic PyTorch Operations\n",
        "\n",
        "### üîç What We're Learning\n",
        "PyTorch tensors are the fundamental data structure for deep learning. They're similar to NumPy arrays but can run on GPUs.\n",
        "\n",
        "### üí° Key Concepts\n",
        "- **Tensors**: Multi-dimensional arrays that can store data and gradients\n",
        "- **Device placement**: Moving tensors between CPU and GPU\n",
        "- **Automatic differentiation**: Computing gradients automatically\n",
        "\n",
        "### üìù Your Tasks\n",
        "Complete the tensor operation exercises below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyeGJVv_cXoR",
        "outputId": "0fc87f9e-a98f-4045-ef28-092d7be5812c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tensor Creation ===\n",
            "List tensor: tensor([1, 2, 3, 4, 5])\n",
            "Zeros shape: torch.Size([3, 4])\n",
            "Random shape: torch.Size([2, 3, 4])\n",
            "Ones like shape: torch.Size([2, 3, 4])\n",
            "\n",
            "=== Tensor Properties ===\n",
            "Shape: torch.Size([3, 4, 5])\n",
            "Data type: torch.float32\n",
            "Device: cpu\n",
            "Number of elements: 60\n",
            "Number of dimensions: 3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"=== Tensor Creation ===\")\n",
        "\n",
        "# TODO: Create different types of tensors\n",
        "# 1. From a Python list\n",
        "list_tensor = torch.tensor([1,2,3,4,5])  # torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "# 2. Zeros tensor of shape (3, 4)\n",
        "zeros_tensor = torch.zeros(3,4)  # torch.zeros(...)\n",
        "\n",
        "# 3. Random tensor of shape (2, 3, 4)\n",
        "random_tensor = torch.randn(2,3,4)  # torch.randn(...)\n",
        "\n",
        "# 4. Tensor of ones with same shape as random_tensor\n",
        "ones_like = torch.ones_like(random_tensor)  # torch.ones_like(...)\n",
        "\n",
        "print(f\"List tensor: {list_tensor}\")\n",
        "print(f\"Zeros shape: {zeros_tensor.shape}\")\n",
        "print(f\"Random shape: {random_tensor.shape}\")\n",
        "print(f\"Ones like shape: {ones_like.shape}\")\n",
        "\n",
        "print(\"\\n=== Tensor Properties ===\")\n",
        "# TODO: Print tensor properties\n",
        "x = torch.randn(3, 4, 5)\n",
        "print(f\"Shape: {x.shape}\")  # x.shape or x.size()\n",
        "print(f\"Data type: {x.dtype}\")  # x.dtype\n",
        "print(f\"Device: {x.device}\")  # x.device\n",
        "print(f\"Number of elements: {x.numel()}\")  # x.numel()\n",
        "print(f\"Number of dimensions: {x.ndim}\")  # x.ndim or len(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0GYrHUTcXoS",
        "outputId": "fa297ab7-91db-44d7-e751-cbba7650788a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tensor Operations ===\n",
            "Tensor a:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "Tensor b:\n",
            "tensor([[5., 6.],\n",
            "        [7., 8.]])\n",
            "\n",
            "Addition:\n",
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]])\n",
            "Element-wise multiplication:\n",
            "tensor([[ 5., 12.],\n",
            "        [21., 32.]])\n",
            "Matrix multiplication:\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "Transpose of a:\n",
            "tensor([[1., 3.],\n",
            "        [2., 4.]])\n",
            "\n",
            "Data tensor:\n",
            "tensor([[ 2.4582,  1.7404, -0.4105],\n",
            "        [-0.1028,  0.4397,  0.9510],\n",
            "        [-0.4304,  1.3314, -0.7306],\n",
            "        [-1.7933,  0.3096,  0.7264]])\n",
            "Mean: 0.3741\n",
            "Std: 1.1678\n",
            "Max: 2.4582\n",
            "Min: -1.7933\n",
            "Sum: 4.4890\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Tensor Operations ===\")  #\n",
        "\n",
        "# Create sample tensors\n",
        "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "\n",
        "print(f\"Tensor a:\\n{a}\")\n",
        "print(f\"Tensor b:\\n{b}\")\n",
        "\n",
        "# TODO: Perform basic operations\n",
        "# 1. Element-wise addition\n",
        "addition = a + b  # a + b or torch.add(a, b)\n",
        "\n",
        "# 2. Element-wise multiplication\n",
        "element_mult = a * b  # a * b\n",
        "\n",
        "# 3. Matrix multiplication\n",
        "matrix_mult = torch.matmul(a, b)  # torch.matmul(a, b) or a @ b\n",
        "\n",
        "# 4. Transpose of tensor a\n",
        "transpose = a.T  # a.T or torch.transpose(a, 0, 1)\n",
        "\n",
        "print(f\"\\nAddition:\\n{addition}\")\n",
        "print(f\"Element-wise multiplication:\\n{element_mult}\")\n",
        "print(f\"Matrix multiplication:\\n{matrix_mult}\")\n",
        "print(f\"Transpose of a:\\n{transpose}\")\n",
        "\n",
        "# TODO: Aggregation operations\n",
        "data = torch.randn(4, 3)\n",
        "print(f\"\\nData tensor:\\n{data}\")\n",
        "\n",
        "# Calculate statistics\n",
        "mean_val = data.mean()  # data.mean()\n",
        "std_val = data.std()   # data.std()\n",
        "max_val = data.max()   # data.max()\n",
        "min_val = data.min()   # data.min()\n",
        "sum_val = data.sum()   # data.sum()\n",
        "\n",
        "print(f\"Mean: {mean_val:.4f}\")\n",
        "print(f\"Std: {std_val:.4f}\")\n",
        "print(f\"Max: {max_val:.4f}\")\n",
        "print(f\"Min: {min_val:.4f}\")\n",
        "print(f\"Sum: {sum_val:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AL-w5EcXoS"
      },
      "source": [
        "### üöÄ Advanced Exercise: Device Movement\n",
        "\n",
        "Practice moving tensors between devices (CPU ‚Üî GPU).\n",
        "\n",
        "**Research Questions:**\n",
        "1. When should you move data to GPU vs keep on CPU?\n",
        "2. What happens if you try to operate on tensors on different devices?\n",
        "3. How does data transfer time compare to computation time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1qwuR7cXoS",
        "outputId": "7130290d-4a01-4671-a309-799acf905139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original device: cpu\n",
            "No GPU available - tensor will stay on CPU\n",
            "üí° In production, always check device availability before moving tensors\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor on CPU\n",
        "cpu_tensor = torch.randn(100, 100)\n",
        "print(f\"Original device: {cpu_tensor.device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # TODO: Move tensor to GPU using different methods\n",
        "    # Method 1: .cuda()\n",
        "    gpu_tensor1 = cpu_tensor.cuda()  # cpu_tensor.cuda()\n",
        "\n",
        "    # Method 2: .to(device)\n",
        "    gpu_tensor2 = cpu_tensor.to('cuda')  # cpu_tensor.to('cuda')\n",
        "\n",
        "    # Method 3: .to(device) with device variable\n",
        "    device = torch.device('cuda')\n",
        "    gpu_tensor3 = cpu_tensor.to(device)  # cpu_tensor.to(device)\n",
        "\n",
        "    print(f\"GPU tensor device: {gpu_tensor1.device}\")\n",
        "\n",
        "    # TODO: Move back to CPU\n",
        "    back_to_cpu = gpu_tensor1.cpu()  # gpu_tensor1.cpu()\n",
        "    print(f\"Back to CPU: {back_to_cpu.device}\")\n",
        "\n",
        "    # TODO: Try operations between tensors on different devices\n",
        "    # This should raise an error - catch it!\n",
        "    try:\n",
        "        result = cpu_tensor + gpu_tensor1  # This will fail!\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error mixing devices: {type(e).__name__}\")\n",
        "        print(f\"Solution: Move both tensors to same device\")\n",
        "\n",
        "        # TODO: Fix by moving both to same device\n",
        "        result = cpu_tensor.cuda() + gpu_tensor1\n",
        "        print(f\"‚úÖ Fixed! Result device: {result.device}\")\n",
        "\n",
        "else:\n",
        "    print(\"No GPU available - tensor will stay on CPU\")\n",
        "    print(\"üí° In production, always check device availability before moving tensors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FGR4-nicXoT"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 4: Environment Best Practices\n",
        "\n",
        "### üìù Reflection Questions\n",
        "\n",
        "Answer these questions based on what you've learned:\n",
        "\n",
        "1. **Why is it important to check your environment before starting an AI project?**\n",
        "   \n",
        "   *Your answer here*\n",
        "\n",
        "2. **What are three advantages of using GPU for deep learning?**\n",
        "   \n",
        "   *Your answer here*\n",
        "\n",
        "3. **What would you do if you ran out of GPU memory during training?**\n",
        "   \n",
        "   *Your answer here*\n",
        "\n",
        "4. **How do you ensure your code works on both CPU and GPU?**\n",
        "   \n",
        "   *Your answer here*\n",
        "\n",
        "### üéØ Environment Checklist\n",
        "\n",
        "Mark off each item as you complete it:\n",
        "\n",
        "- [ ] Python 3.10+ verified\n",
        "- [ ] All required packages imported successfully  \n",
        "- [ ] GPU availability checked\n",
        "- [ ] Basic tensor operations completed\n",
        "- [ ] Device movement practiced\n",
        "- [ ] Performance comparison run\n",
        "- [ ] Understanding of PyTorch basics confirmed\n",
        "\n",
        "### üîß Troubleshooting Common Issues\n",
        "\n",
        "If you encountered problems, research and note solutions here:\n",
        "\n",
        "**Import Errors:**\n",
        "- Problem: `ModuleNotFoundError: No module named 'torch'`\n",
        "- Solution: *Your research here*\n",
        "\n",
        "**CUDA Issues:**\n",
        "- Problem: `CUDA out of memory`\n",
        "- Solution: *Your research here*\n",
        "\n",
        "**Performance Issues:**\n",
        "- Problem: GPU slower than expected\n",
        "- Solution: *Your research here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBWUaLw9cXoT"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ Checkpoint Complete!\n",
        "\n",
        "**What You've Accomplished:**\n",
        "- ‚úÖ Verified your development environment\n",
        "- ‚úÖ Understood hardware requirements for AI\n",
        "- ‚úÖ Learned basic PyTorch tensor operations\n",
        "- ‚úÖ Practiced device management (CPU/GPU)\n",
        "- ‚úÖ Identified potential issues and solutions\n",
        "\n",
        "**Key Takeaways:**\n",
        "1. Environment consistency is critical for reproducible AI\n",
        "2. GPU acceleration can dramatically speed up deep learning\n",
        "3. PyTorch tensors are the foundation of modern deep learning\n",
        "4. Always check device compatibility in your code\n",
        "\n",
        "**Next Steps:**\n",
        "- Continue to the AI Refresher notebook\n",
        "- Keep your environment active for the rest of Day 1\n",
        "- Ask questions if anything wasn't clear!\n",
        "\n",
        "---\n",
        "\n",
        "**üìö Additional Resources to Explore:**\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "- [CUDA Programming Guide](https://docs.nvidia.com/cuda/)\n",
        "- [Virtual Environments Guide](https://docs.python.org/3/tutorial/venv.html)\n",
        "\n",
        "*Environment Setup Complete - Ready for AI Deep Dive!* üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut6BwGxPcXQ",
        "outputId": "b3a1d81f-f0fb-4626-a2f7-7f7578b87dd5"
      },
      "source": [
        "import time\n",
        "import psutil  # You may need to install this: pip install psutil\n",
        "\n",
        "# System RAM check\n",
        "ram_info = psutil.virtual_memory()\n",
        "print(f\"Total RAM: {ram_info.total / (1024**3):.1f} GB\")\n",
        "print(f\"Available RAM: {ram_info.available / (1024**3):.1f} GB\")\n",
        "\n",
        "# GPU memory check (if available)\n",
        "if cuda_available:\n",
        "    # TODO: Get GPU memory info using torch.cuda methods\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory  # torch.cuda.get_device_properties(0).total_memory\n",
        "    allocated_memory = torch.cuda.memory_allocated(0)  # torch.cuda.memory_allocated(0)\n",
        "\n",
        "    print(f\"\\nGPU Total Memory: {total_memory / (1024**3):.1f} GB\")\n",
        "    print(f\"GPU Allocated Memory: {allocated_memory / (1024**3):.1f} GB\")\n",
        "\n",
        "# Simple performance test\n",
        "print(\"\\nüî¨ Running performance test...\")\n",
        "\n",
        "# TODO: Create two large random tensors\n",
        "size = 1000\n",
        "a = torch.randn(size, size)  # Create on CPU first\n",
        "b = torch.randn(size, size)\n",
        "\n",
        "# CPU timing\n",
        "start_time = time.time()\n",
        "# TODO: Perform matrix multiplication on CPU\n",
        "result_cpu = torch.matmul(a, b)  # torch.matmul or @ operator\n",
        "cpu_time = time.time() - start_time\n",
        "\n",
        "print(f\"CPU computation time: {cpu_time:.4f} seconds\")\n",
        "\n",
        "# GPU timing (if available)\n",
        "if cuda_available:\n",
        "    # TODO: Move tensors to GPU\n",
        "    a_gpu = a.to(device)  # tensor.to(device) or tensor.cuda()\n",
        "    b_gpu = b.to(device)\n",
        "\n",
        "    # TODO: Warm up GPU (run operation once)\n",
        "    _ = torch.matmul(a_gpu, b_gpu)\n",
        "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "\n",
        "    start_time = time.time()\n",
        "    # TODO: Perform matrix multiplication on GPU\n",
        "    result_gpu = torch.matmul(a_gpu, b_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "\n",
        "    print(f\"GPU computation time: {gpu_time:.4f} seconds\")\n",
        "    print(f\"Speedup: {cpu_time/gpu_time:.1f}x faster on GPU\")\n",
        "\n",
        "print(\"‚úÖ Performance test complete!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total RAM: 12.7 GB\n",
            "Available RAM: 11.4 GB\n",
            "\n",
            "üî¨ Running performance test...\n",
            "CPU computation time: 0.1497 seconds\n",
            "‚úÖ Performance test complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}