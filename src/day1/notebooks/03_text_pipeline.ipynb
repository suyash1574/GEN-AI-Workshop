{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyash1574/GEN-AI-Workshop/blob/main/src/day1/notebooks/03_text_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lehT7-vBigZb"
      },
      "source": [
        "# Day 1: Text Pipeline - Your First Language Model\n",
        "\n",
        "Welcome to hands-on text processing! Now that you understand neural networks, let's explore how they work with text data.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "- Understand how text becomes numbers (tokenization)\n",
        "- Load and use a pre-trained language model\n",
        "- Experiment with text generation parameters\n",
        "- Compare different prompt engineering techniques\n",
        "- Build your first text generation pipeline\n",
        "\n",
        "## üìö Research Focus\n",
        "This notebook emphasizes **discovery learning**. You'll:\n",
        "1. Research concepts before implementing\n",
        "2. Experiment with parameters to see their effects\n",
        "3. Compare different approaches\n",
        "4. Build understanding through hands-on exploration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T66KHTnGigZg"
      },
      "source": [
        "## 1. From Text to Numbers\n",
        "\n",
        "Neural networks work with numbers, but we have text. How do we bridge this gap?\n",
        "\n",
        "üîç **RESEARCH TASK 1**:\n",
        "- What is tokenization in NLP?\n",
        "- What is the difference between word-level and sub-word tokenization?\n",
        "- Research \"BPE\" (Byte Pair Encoding) - how does it work?\n",
        "- Why can't we just assign each word a number?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzgU0amLigZh",
        "outputId": "7e61d2fe-74f1-4e08-be20-30d842940c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XHZYarpigZj"
      },
      "source": [
        "### Exploring Tokenization\n",
        "\n",
        "üîç **RESEARCH TASK 2**:\n",
        "- Look up the GPT-2 tokenizer documentation\n",
        "- What is a \"vocabulary size\"?\n",
        "- What happens when the model encounters a word it's never seen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-okZ4tETigZk",
        "outputId": "8f1b40bc-7099-4e02-ea6f-179ca1cbf2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Exploring Tokenization:\n",
            "==================================================\n",
            "\n",
            "Original: Hello world!\n",
            "Tokens: ['Hello', 'ƒ†world', '!']\n",
            "Token IDs: [15496, 995, 0]\n",
            "Number of tokens: 3\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Tokens: ['The', 'ƒ†quick', 'ƒ†brown', 'ƒ†fox', 'ƒ†jumps', 'ƒ†over', 'ƒ†the', 'ƒ†lazy', 'ƒ†dog', '.']\n",
            "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Number of tokens: 10\n",
            "\n",
            "Original: Artificial intelligence is revolutionizing technology.\n",
            "Tokens: ['Art', 'ificial', 'ƒ†intelligence', 'ƒ†is', 'ƒ†revolution', 'izing', 'ƒ†technology', '.']\n",
            "Token IDs: [8001, 9542, 4430, 318, 5854, 2890, 3037, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: GPT-2 uses transformer architecture.\n",
            "Tokens: ['G', 'PT', '-', '2', 'ƒ†uses', 'ƒ†transformer', 'ƒ†architecture', '.']\n",
            "Token IDs: [38, 11571, 12, 17, 3544, 47385, 10959, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: Supercalifragilisticexpialidocious\n",
            "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
            "Token IDs: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
            "Number of tokens: 11\n",
            "\n",
            "üìä Tokenizer vocabulary size: 50257\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the GPT-2 tokenizer\n",
        "# Hint: Use GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Test sentences to explore tokenization\n",
        "test_sentences = [\n",
        "    \"Hello world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is revolutionizing technology.\",\n",
        "    \"GPT-2 uses transformer architecture.\",\n",
        "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
        "]\n",
        "\n",
        "print(\"üîç Exploring Tokenization:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # TODO: Tokenize the sentence\n",
        "    # Hint: Use tokenizer.encode() to get token IDs\n",
        "    # Use tokenizer.tokenize() to see the actual tokens\n",
        "    tokens = tokenizer.tokenize(sentence)  # Get the actual token strings\n",
        "    token_ids = tokenizer.encode(sentence)  # Get the numerical IDs\n",
        "\n",
        "    print(f\"\\nOriginal: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# TODO: Print tokenizer vocabulary size\n",
        "print(f\"\\nüìä Tokenizer vocabulary size: {len(tokenizer)}\")  # Hint: len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKXGzFcigZl"
      },
      "source": [
        "### Understanding Token Patterns\n",
        "\n",
        "üîç **RESEARCH TASK 3**:\n",
        "- Why do some words get split into multiple tokens?\n",
        "- What does the 'ƒ†' symbol represent in GPT-2 tokens?\n",
        "- How might tokenization affect model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAjiAzvxigZl",
        "outputId": "2538329e-ae1a-42dc-b761-b33f2e8293a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Token Pattern Analysis:\n",
            "============================================================\n",
            "running                        ‚Üí ['running'] (1 tokens)\n",
            "runner                         ‚Üí ['runner'] (1 tokens)\n",
            "run                            ‚Üí ['run'] (1 tokens)\n",
            "unhappiness                    ‚Üí ['un', 'h', 'appiness'] (3 tokens)\n",
            "ChatGPT                        ‚Üí ['Chat', 'G', 'PT'] (3 tokens)\n",
            "COVID-19                       ‚Üí ['CO', 'VID', '-', '19'] (4 tokens)\n",
            "2023                           ‚Üí ['20', '23'] (2 tokens)\n",
            "programming                    ‚Üí ['program', 'ming'] (2 tokens)\n",
            "antidisestablishmentarianism   ‚Üí ['ant', 'idis', 'establishment', 'arian', 'ism'] (5 tokens)\n",
            "\n",
            "üìä Average characters per token: 4.12\n",
            "üìä Longest word in tokens: antidisestablishmentarianism\n"
          ]
        }
      ],
      "source": [
        "# Analyze tokenization patterns\n",
        "analysis_texts = [\n",
        "    \"running\",\n",
        "    \"runner\",\n",
        "    \"run\",\n",
        "    \"unhappiness\",\n",
        "    \"ChatGPT\",\n",
        "    \"COVID-19\",\n",
        "    \"2023\",\n",
        "    \"programming\",\n",
        "    \"antidisestablishmentarianism\"\n",
        "]\n",
        "\n",
        "print(\"üîç Token Pattern Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "token_analysis = []\n",
        "\n",
        "for text in analysis_texts:\n",
        "    # TODO: Analyze each text\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
        "    token_ids = tokenizer.encode(text)  # Get token IDs\n",
        "    token_count = len(token_ids) # Count the tokens\n",
        "\n",
        "\n",
        "    token_analysis.append({\n",
        "        'text': text,\n",
        "        'tokens': tokens,\n",
        "        'token_count': token_count,\n",
        "        'chars_per_token': len(text) / token_count if token_count > 0 else 0 # Avoid division by zero\n",
        "    })\n",
        "\n",
        "    print(f\"{text:30} ‚Üí {tokens} ({token_count} tokens)\")\n",
        "\n",
        "# TODO: Create a DataFrame and analyze patterns\n",
        "df = pd.DataFrame(token_analysis)\n",
        "print(f\"\\nüìä Average characters per token: {df['chars_per_token'].mean():.2f}\")  # Calculate mean\n",
        "print(f\"üìä Longest word in tokens: {df.loc[df['token_count'].idxmax()]['text']}\")  # Find max token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsvgPrQiigZl"
      },
      "source": [
        "## 2. Loading Your First Language Model\n",
        "\n",
        "Now let's load GPT-2 and understand its architecture.\n",
        "\n",
        "üîç **RESEARCH TASK 4**:\n",
        "- What is GPT-2 and when was it released?\n",
        "- How many parameters does GPT-2 have? (Compare different sizes)\n",
        "- What is \"autoregressive\" text generation?\n",
        "- How does GPT-2 relate to the neural network you built in the previous notebook?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "47382dfa89b4401584297b0fca397521",
            "e830e0df42c6488185f3b8959117e40b",
            "b5ce44c720ed46f8892646a5ac1b42d0",
            "1547720d54ba414ca40898ea32117a5d",
            "ae7d79f9c21e4fb68f26286b6a3f1166",
            "dc725d3945514e699a87ecc5b88370cc",
            "1b6228f46dba4febabbea8e96bfd7bee",
            "92b169fa52704163b9ae7bf43c58ccf0",
            "97412157b14c48ae89c2ace9f007e29e",
            "c9647f1d40fc4cc09cae080af44cd743",
            "dd52a0a9b8394f6ca0e8a79cdecd1f0a",
            "164294e677204a5fa6e85fa4034d8469",
            "14323bb6a2574eccb4ba6b7e49bbfea5",
            "2e34720695734be3afc83a584c423548",
            "550f847c918b4973abc0a8f7e8a87384",
            "ed21bae00cab4eb0a70b2a93459bf9dd",
            "fefce3b40bbb4a058406fb1ea9d40ae3",
            "f78bbeae508242b6bef841ef3f8d36ee",
            "0863b8914dc54b6fbd07fb831de90333",
            "af05b57838894357a8ae9cee8a1e42f0",
            "bd29ab040d51413aa5b0b1dd932d971f",
            "6051186e33b0427584072ffa66be58e1"
          ]
        },
        "id": "sRQI8fwHigZm",
        "outputId": "dfe6d404-87d5-43b4-88bc-f69cb3989d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GPT-2 model (this may take a moment)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47382dfa89b4401584297b0fca397521"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "164294e677204a5fa6e85fa4034d8469"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPT-2 model loaded successfully!\n",
            "\n",
            "üèóÔ∏è Model Architecture:\n",
            "Model type: GPT2LMHeadModel\n",
            "Total parameters: 124,439,808\n",
            "Model size: ~124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load GPT-2 model\n",
        "# Hint: Use GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"üîÑ Loading GPT-2 model (this may take a moment)...\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: Use model.eval()\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ GPT-2 model loaded successfully!\")\n",
        "\n",
        "# Explore model architecture\n",
        "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "\n",
        "# TODO: Count model parameters\n",
        "# Hint: sum(p.numel() for p in model.parameters())\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSDWHntfigZm"
      },
      "source": [
        "### Understanding Model Architecture\n",
        "\n",
        "üîç **RESEARCH TASK 5**:\n",
        "- What are \"transformer blocks\" in GPT-2?\n",
        "- What is \"attention\" in the context of neural networks?\n",
        "- How does this compare to the simple network you built earlier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6aIdJfBigZm",
        "outputId": "d06abb27-c505-48d5-f03f-f69700b2b3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Model Structure Analysis:\n",
            "==================================================\n",
            "Vocabulary size: 50257\n",
            "Maximum sequence length: 1024\n",
            "Number of transformer layers: 12\n",
            "Number of attention heads: 12\n",
            "Hidden size: 768\n",
            "\n",
            "ü§î Comparison to Your Neural Network:\n",
            "Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\n",
            "GPT-2 has: 50257 inputs ‚Üí 768 hidden ‚Üí 50257 outputs\n",
            "Your network: ~50 parameters\n",
            "GPT-2: 124,439,808 parameters\n",
            "GPT-2 is ~2,488,796x larger!\n"
          ]
        }
      ],
      "source": [
        "# Explore model structure\n",
        "print(\"üîç Model Structure Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Print model configuration\n",
        "# Hint: Use model.config\n",
        "config = model.config\n",
        "\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Maximum sequence length: {config.n_positions}\")\n",
        "print(f\"Number of transformer layers: {config.n_layer}\")\n",
        "print(f\"Number of attention heads: {config.n_head}\")\n",
        "print(f\"Hidden size: {config.n_embd}\")\n",
        "\n",
        "# Compare to your simple network\n",
        "print(\"\\nü§î Comparison to Your Neural Network:\")\n",
        "print(f\"Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\")\n",
        "print(f\"GPT-2 has: {config.vocab_size} inputs ‚Üí {config.n_embd} hidden ‚Üí {config.vocab_size} outputs\")\n",
        "print(f\"Your network: ~50 parameters\")\n",
        "print(f\"GPT-2: {total_params:,} parameters\")\n",
        "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSsrEUG_igZn"
      },
      "source": [
        "## 3. Text Generation Experiments\n",
        "\n",
        "Let's generate text and understand how different parameters affect the output.\n",
        "\n",
        "üîç **RESEARCH TASK 6**:\n",
        "- What is \"temperature\" in text generation?\n",
        "- What is \"top-p\" (nucleus) sampling?\n",
        "- What's the difference between greedy decoding and sampling?\n",
        "- How do these parameters affect creativity vs. coherence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5YoE1J6igZn",
        "outputId": "e5af9377-ff3c-408b-e06e-b3f22a84eba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Base prompt: 'In the future, artificial intelligence will'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a text generation pipeline\n",
        "# Hint: Use pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "generator =pipeline('text-generation' , model=model , tokenizer=tokenizer)\n",
        "\n",
        "# Base prompt for experiments\n",
        "base_prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "print(f\"ü§ñ Base prompt: '{base_prompt}'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHh8t9CigZn"
      },
      "source": [
        "### Temperature Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 7**:\n",
        "- What happens when temperature = 0?\n",
        "- What happens when temperature > 1?\n",
        "- Why might you want different temperatures for different tasks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRz5quFoigZn",
        "outputId": "bb16adaf-ca37-43e7-9ffe-a2bf3cd987e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå°Ô∏è Temperature Experiments:\n",
            "==================================================\n",
            "\n",
            "üî• Temperature: 0.1\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more\n",
            "\n",
            "üî• Temperature: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will do a lot more than just figure out the right way to handle the problem of what needs to be solved. That's why we need to invest in artificial intelligence so that we can find the best ways to solve the problem of what needs to be solved.\n",
            "\n",
            "How will AI work with human intelligence?\n",
            "\n",
            "The AI will play a critical role in this. The AI will be able to do the work needed to solve a problem. The AI will help with the processes that are needed to do the work needed to solve a problem.\n",
            "\n",
            "The AI will be able to solve a problem in a way that is not constrained by human limitations. It will be able to solve a problem in a way that is not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits. It will be able to solve a problem that is not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits.\n",
            "\n",
            "So AI will be able to solve problems that are not constrained by human limitations. It will be able to solve problems that are not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits. It will be able to solve a problem that is\n",
            "\n",
            "üî• Temperature: 1.0\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will grow to be powerful, but also scalable to every sector of the military and government.\n",
            "\n",
            "The U.S. believes that the world will soon come together to make sure the next generation of computers does indeed be ready today so that it can do everything necessary to become better at tasks such as intelligence, transportation, financial, communications technologies and energy.\n",
            "\n",
            "But for now, the world will be left with artificial intelligence. What kind of information can you use to do good things today, or to do better tomorrow?\n",
            "\n",
            "In November 2010, the U.S. General Services Administration asked the Defense Energy Research and Development Agency to ask for a $3.16 billion contract for a new U.S. military industrial base in the Indian Ocean.\n",
            "\n",
            "It has not been officially approved. But the U.S. government has long been known for getting things done with computers. The government even allowed the U.S. Navy to put computers in naval vessels. Those efforts have now been shut down amid delays of orders from the Obama administration to open the C-130 cargo ship to foreign customers.\n",
            "\n",
            "A year and half ago, the Department of Transportation gave two government agencies permission, the National Transportation Safety Board (NTSB), to study the impact of U\n",
            "\n",
            "üî• Temperature: 1.5\n",
            "------------------------------\n",
            "In the future, artificial intelligence will help build tools, services and applications to do that while maintaining the user experience. The goal of \"computing intelligent assistants\" will also vary depending on many different industries and industries. In this post we outline all that we'll focus on since our research suggests that AI algorithms to understand a person (computer scientist) might or might not be all that common in recent times. Please note, these recommendations assume at least some experience at least one job. The most common ones cited were those who held a PhD in biological (science, surgery, philosophy or health sciences and some business or business investment management positions including insurance)\n",
            "\n",
            "Research shows many types of computing, but mostly computing in the data and analysis arena\n",
            "\n",
            "Some scientists even have specialized areas of expertise that AI might work as long as there was little or no chance that AI would change in order to create an efficient machine\n",
            "\n",
            "\n",
            "These skills would not be common by now because people have already learned enough how AI-powered technologies can improve. However, many scientists say we are all beginning to understand how to implement \"computing intelligently\", so it seems worth going through these recommendations in full and looking.\n",
            "\n",
            "These algorithms should support the following things‚Ä¶\n",
            "\n",
            "What you do with AI\n",
            "\n",
            "\n",
            "This may not include using machine\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ Which temperature produced the most coherent text?\n",
            "‚Ä¢ Which was most creative/surprising?\n",
            "‚Ä¢ When might you use each temperature setting?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different temperatures\n",
        "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
        "\n",
        "print(\"üå°Ô∏è Temperature Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nüî• Temperature: {temp}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different temperatures\n",
        "    # Hint: Use generator() with temperature parameter\n",
        "    result = generator(\n",
        "        base_prompt,  # prompt\n",
        "        max_length=60,  # try 60\n",
        "        temperature=temp,  # use the temp variable\n",
        "        do_sample=True,  # should be True for sampling\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # TODO: Print the generated text\n",
        "    generated_text = result[0]['generated_text']  # Extract from result\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ Which temperature produced the most coherent text?\")\n",
        "print(\"‚Ä¢ Which was most creative/surprising?\")\n",
        "print(\"‚Ä¢ When might you use each temperature setting?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PGFsaXDigZo"
      },
      "source": [
        "### Top-p (Nucleus) Sampling Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 8**:\n",
        "- How does top-p sampling work?\n",
        "- What's the difference between top-k and top-p sampling?\n",
        "- Why might top-p be better than just using temperature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ7c1yiBigZo",
        "outputId": "f5451c18-0e49-4a02-804b-9df5977ef2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Top-p Sampling Experiments:\n",
            "==================================================\n",
            "\n",
            "üé≤ Top-p: 0.3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to help us better understand our own lives, and to help us better understand others.\n",
            "\n",
            "The future of AI is a very exciting one. It is a new era in which we are seeing the emergence of new technologies that will change the way we think, act, and think.\n",
            "\n",
            "The future of AI is a very exciting one. It is a new era in which we are seeing the emergence of new technologies that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act\n",
            "\n",
            "üé≤ Top-p: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to be used to solve problems of complexity, intelligence, and intelligence-based solutions.\n",
            "\n",
            "The \"Cognitive Science of Artificial Intelligence\" (CSI) is a research and development project that aims to develop the ability to design, build, and deploy cognitive systems. CSI will be led by the AI Lab at the University of Toronto. The project is being led by the AI Lab at the University of Toronto.\n",
            "\n",
            "The CSI will focus on understanding how the human brain works, how it operates, and how it interacts with the environment. It will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial\n",
            "\n",
            "üé≤ Top-p: 0.9\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to work with many of the same features as humans, including spatial navigation and the ability to control computers.\n",
            "\n",
            "The team has also explored the possibility of using AI for scientific research, which will include the creation of intelligent robots and robots that would be capable of performing tasks that humans could not.\n",
            "\n",
            "\"We are also interested in using robots to help people understand their environment, to help them develop new skills, to create new products, and to find new solutions to our problems,\" said Paul Shultz, head of AI at Google. \"This is one of the most exciting opportunities for humans to explore.\"\n",
            "\n",
            "In the meantime, the team plans to continue to develop its AI projects. In the future, they will be able to use robots to perform tasks that humans could not.\n",
            "\n",
            "The team has also developed AI software that would be used to identify and solve problems using real-world information, such as data on the weather, traffic patterns, and temperature.\n",
            "\n",
            "In the future, the team is also working on other applications of AI for real-world research.\n",
            "\n",
            "üé≤ Top-p: 1.0\n",
            "------------------------------\n",
            "In the future, artificial intelligence will have a new home in the hands of a company called DeepMind. One of their goals is to create robots capable of solving complex problems, such as solving some complex logic puzzles.\n",
            "\n",
            "A third goal is to develop new machines capable of solving some of the most complex problems, such as a complex math problem.\n",
            "\n",
            "The AI Lab is set to present the first Artificial Intelligence Conference in Singapore, which starts in July, and is expected to become a major focus of the international AI conference.\n",
            "\n",
            "The AI Lab's main goal is to develop and test a new type of AI system, called a machine learning system, which can learn from human-related knowledge and learn from other AI systems.\n",
            "\n",
            "The AI Lab will also help in developing a new system for artificial intelligence or AI applications, in which the artificial intelligence will use natural language processing to learn from natural language processing.\n",
            "\n",
            "AI Lab Director-General K. S. Gupta said the AI Lab had identified the key areas in which AI could go wrong and it was important to develop technologies that would help improve AI.\n",
            "\n",
            "He said the Artificial Intelligence Lab was a start, but that AI research would continue.\n",
            "\n",
            "\"AI does not have to be a science,\" said Gupta. \"If it is important\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ How did the outputs change with different top-p values?\n",
            "‚Ä¢ What's the trade-off between diversity and quality?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with top-p sampling\n",
        "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
        "\n",
        "print(\"üéØ Top-p Sampling Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for top_p in top_p_values:\n",
        "    print(f\"\\nüé≤ Top-p: {top_p}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different top-p values\n",
        "    result = generator(\n",
        "        base_prompt,\n",
        "        max_length=60,\n",
        "        temperature=0.8,  # Keep temperature constant\n",
        "        top_p=top_p,  # Use the top_p variable\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ How did the outputs change with different top-p values?\")\n",
        "print(\"‚Ä¢ What's the trade-off between diversity and quality?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXLK5NpigZo"
      },
      "source": [
        "## 4. Prompt Engineering Experiments\n",
        "\n",
        "The way you phrase your prompt dramatically affects the output.\n",
        "\n",
        "üîç **RESEARCH TASK 9**:\n",
        "- What is \"prompt engineering\"?\n",
        "- What are \"few-shot\" prompts?\n",
        "- How can prompt structure influence model behavior?\n",
        "- Research common prompt engineering techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnERVPBEigZo"
      },
      "outputs": [],
      "source": [
        "# Different prompt styles to experiment with\n",
        "prompts_to_test = {\n",
        "    \"Direct\": \"Write about artificial intelligence:\",\n",
        "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
        "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
        "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
        "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
        "    \"Few_Shot\": \"Technology predictions:\\n‚Ä¢ The internet will connect everyone (1990s)\\n‚Ä¢ Smartphones will be everywhere (2000s)\\n‚Ä¢ Artificial intelligence will\"\n",
        "}\n",
        "\n",
        "print(\"‚úçÔ∏è Prompt Engineering Experiments:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# TODO: Test each prompt style\n",
        "for style, prompt in prompts_to_test.items():\n",
        "    print(f\"\\nüìù Style: {style}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate text for each prompt\n",
        "    result = generator(\n",
        "        ____,  # use the prompt variable\n",
        "        max_length=80,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQkexDsBigZo"
      },
      "source": [
        "### Analyzing Prompt Effectiveness\n",
        "\n",
        "üîç **RESEARCH TASK 10**:\n",
        "- Which prompt style produced the most useful output?\n",
        "- How did the model's \"behavior\" change with different prompts?\n",
        "- What makes a good prompt?\n",
        "- How might this apply to chatbots or AI assistants?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8brmkwsigZp"
      },
      "outputs": [],
      "source": [
        "# Let's analyze the generated text more systematically\n",
        "print(\"üìä Prompt Analysis Exercise:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: For each prompt style, generate multiple outputs and analyze\n",
        "analysis_results = []\n",
        "\n",
        "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
        "    # Generate 3 outputs for each prompt\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        # TODO: Generate text\n",
        "        result = generator(\n",
        "            prompt,\n",
        "            max_length=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        output = result[0]['generated_text']\n",
        "        outputs.append(output)\n",
        "\n",
        "    # TODO: Analyze the outputs\n",
        "    lengths = ___\n",
        "    avg_length = ___ # Calculate average length of outputs\n",
        "\n",
        "    analysis_results.append({\n",
        "        'style': style,\n",
        "        'prompt': prompt,\n",
        "        'avg_length': avg_length,\n",
        "        'outputs': outputs\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{style}:\")\n",
        "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
        "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
        "\n",
        "print(\"\\nü§î Reflection Questions:\")\n",
        "print(\"‚Ä¢ Which prompt style was most consistent?\")\n",
        "print(\"‚Ä¢ Which produced the most relevant outputs?\")\n",
        "print(\"‚Ä¢ How might you improve these prompts?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvObchkgigZp"
      },
      "source": [
        "## 5. Building Your Text Generation Pipeline\n",
        "\n",
        "Now let's create a customizable text generation function.\n",
        "\n",
        "üîç **RESEARCH TASK 11**:\n",
        "- What parameters should a good text generation function have?\n",
        "- How can you make text generation more controllable?\n",
        "- What are the trade-offs between different generation strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SO8TzznigZq"
      },
      "outputs": [],
      "source": [
        "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
        "    \"\"\"\n",
        "    TODO: Create a customizable text generation function\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt\n",
        "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
        "        length (str): \"short\", \"medium\", or \"long\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Set parameters based on style\n",
        "    if style == \"creative\":\n",
        "        temperature = ____  # Higher for creativity\n",
        "        top_p = ____        # Higher for diversity\n",
        "    elif style == \"conservative\":\n",
        "        temperature = ____  # Lower for consistency\n",
        "        top_p = ____        # Lower for focus\n",
        "    else:  # balanced\n",
        "        temperature = ____  # Medium values\n",
        "        top_p = ____\n",
        "\n",
        "    # TODO: Set length based on parameter\n",
        "    if length == \"short\":\n",
        "        max_length = ____  # Try 40\n",
        "    elif length == \"long\":\n",
        "        max_length = ____  # Try 100\n",
        "    else:  # medium\n",
        "        max_length = ____  # Try 70\n",
        "\n",
        "    # TODO: Generate text with the parameters\n",
        "    result = generator(\n",
        "        ____,  # prompt\n",
        "        max_length=____,\n",
        "        temperature=____,\n",
        "        top_p=____,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Test your function\n",
        "test_prompt = \"The future of education will be\"\n",
        "\n",
        "print(\"üß™ Testing Your Text Generator:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Test different combinations\n",
        "test_combinations = [\n",
        "    (\"creative\", \"short\"),\n",
        "    (\"balanced\", \"medium\"),\n",
        "    (\"conservative\", \"long\")\n",
        "]\n",
        "\n",
        "for style, length in test_combinations:\n",
        "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Use your function\n",
        "    output = custom_text_generator(____)\n",
        "    print(output)\n",
        "    print(f\"Characters: {len(output)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVt6HX5figZq"
      },
      "source": [
        "## 6. Creative Applications\n",
        "\n",
        "Let's explore some creative uses of text generation.\n",
        "\n",
        "üîç **RESEARCH TASK 12**:\n",
        "- How is GPT-2 being used in creative writing?\n",
        "- What are some potential applications for businesses?\n",
        "- What ethical considerations should we keep in mind?\n",
        "- How might this technology evolve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Ouhfj4igZq"
      },
      "outputs": [],
      "source": [
        "# Creative applications to try\n",
        "creative_prompts = {\n",
        "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
        "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
        "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
        "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
        "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
        "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
        "}\n",
        "\n",
        "print(\"üé® Creative Applications:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Generate creative content\n",
        "for app_type, prompt in creative_prompts.items():\n",
        "    print(f\"\\nüñºÔ∏è {app_type}:\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Choose appropriate style for each application\n",
        "    if app_type in [\"Poetry\", \"Story\"]:\n",
        "        style = ____  # Should be creative\n",
        "    elif app_type in [\"Product Description\", \"Email\"]:\n",
        "        style = ____  # Should be conservative\n",
        "    else:\n",
        "        style = ____  # Should be balanced\n",
        "\n",
        "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
        "    print(output)\n",
        "    print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3MaAlsJigZq"
      },
      "source": [
        "## 7. Understanding Limitations\n",
        "\n",
        "It's important to understand what language models can and cannot do.\n",
        "\n",
        "üîç **RESEARCH TASK 13**:\n",
        "- What is \"hallucination\" in language models?\n",
        "- Why might GPT-2 generate biased or incorrect information?\n",
        "- What are the limitations of autoregressive generation?\n",
        "- How do these limitations affect real-world applications?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXELGphPigZq"
      },
      "outputs": [],
      "source": [
        "# Test model limitations\n",
        "limitation_tests = {\n",
        "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
        "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
        "    \"Math\": \"What is 47 * 83? The answer is\",\n",
        "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
        "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
        "}\n",
        "\n",
        "print(\"‚ö†Ô∏è Understanding Model Limitations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for test_type, prompt in limitation_tests.items():\n",
        "    print(f\"\\nüß™ Testing: {test_type}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate responses to test limitations\n",
        "    output = custom_text_generator(\n",
        "        ____,  # prompt\n",
        "        style=\"conservative\",  # Use conservative for factual tasks\n",
        "        length=\"short\"\n",
        "    )\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    # TODO: Analyze the output\n",
        "    print(f\"ü§î Analysis: Does this look correct/reasonable?\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Important Reminders:\")\n",
        "print(\"‚Ä¢ Language models can generate plausible-sounding but incorrect information\")\n",
        "print(\"‚Ä¢ Always verify factual claims from AI-generated content\")\n",
        "print(\"‚Ä¢ Be aware of potential biases in training data\")\n",
        "print(\"‚Ä¢ Use AI as a tool to assist, not replace, human judgment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axckvie5igZr"
      },
      "source": [
        "## 8. Reflection and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "‚úÖ **Understood tokenization and text preprocessing**\n",
        "‚úÖ **Loaded and used a pre-trained language model**\n",
        "‚úÖ **Experimented with generation parameters**\n",
        "‚úÖ **Explored prompt engineering techniques**\n",
        "‚úÖ **Built a customizable text generation pipeline**\n",
        "‚úÖ **Understood model limitations and ethical considerations**\n",
        "\n",
        "### Key Insights\n",
        "üîç **Discussion Questions**:\n",
        "- What surprised you most about text generation?\n",
        "- Which prompt engineering technique was most effective?\n",
        "- How might you use this in a real project?\n",
        "- What limitations concerned you most?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxIQBWtJigZr"
      },
      "outputs": [],
      "source": [
        "# Final experiment: Design your own use case\n",
        "print(\"üéØ FINAL CHALLENGE:\")\n",
        "print(\"Design your own text generation use case!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Create your own application\n",
        "# Ideas: Story generator, email assistant, creative writing helper, etc.\n",
        "\n",
        "your_use_case = \"____\"  # Describe your use case\n",
        "your_prompt = \"____\"   # Design your prompt\n",
        "your_style = \"____\"    # Choose your style\n",
        "your_length = \"____\"   # Choose your length\n",
        "\n",
        "print(f\"üìù Your use case: {your_use_case}\")\n",
        "print(f\"üìù Your prompt: '{your_prompt}'\")\n",
        "print(f\"üìù Your settings: {your_style}, {your_length}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# TODO: Generate with your custom settings\n",
        "your_output = custom_text_generator(____)\n",
        "print(\"üéâ Your generated content:\")\n",
        "print(your_output)\n",
        "\n",
        "print(\"\\nüìà Next Steps:\")\n",
        "print(\"‚Ä¢ Experiment with different prompt formats\")\n",
        "print(\"‚Ä¢ Try combining multiple generation calls\")\n",
        "print(\"‚Ä¢ Think about how to validate or improve outputs\")\n",
        "print(\"‚Ä¢ Consider user interface design for your application\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loqO5x9MigZr"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "- ‚úÖ Mastered text tokenization and preprocessing\n",
        "- ‚úÖ Used a state-of-the-art language model\n",
        "- ‚úÖ Discovered the art and science of prompt engineering\n",
        "- ‚úÖ Built your own text generation pipeline\n",
        "- ‚úÖ Understood the capabilities and limitations of AI text generation\n",
        "- ‚úÖ Explored creative applications\n",
        "\n",
        "### Prepare for the Next Notebook\n",
        "Next, we'll explore computer vision and image processing, applying similar principles to visual data!\n",
        "\n",
        "**Share with your partner**: What was your most successful text generation experiment?\n",
        "\n",
        "---\n",
        "*Text Pipeline Complete - Ready for Computer Vision! üñºÔ∏è*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47382dfa89b4401584297b0fca397521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e830e0df42c6488185f3b8959117e40b",
              "IPY_MODEL_b5ce44c720ed46f8892646a5ac1b42d0",
              "IPY_MODEL_1547720d54ba414ca40898ea32117a5d"
            ],
            "layout": "IPY_MODEL_ae7d79f9c21e4fb68f26286b6a3f1166"
          }
        },
        "e830e0df42c6488185f3b8959117e40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc725d3945514e699a87ecc5b88370cc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b6228f46dba4febabbea8e96bfd7bee",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "b5ce44c720ed46f8892646a5ac1b42d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92b169fa52704163b9ae7bf43c58ccf0",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97412157b14c48ae89c2ace9f007e29e",
            "value": 548105171
          }
        },
        "1547720d54ba414ca40898ea32117a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9647f1d40fc4cc09cae080af44cd743",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dd52a0a9b8394f6ca0e8a79cdecd1f0a",
            "value": "‚Äá548M/548M‚Äá[00:25&lt;00:00,‚Äá20.1MB/s]"
          }
        },
        "ae7d79f9c21e4fb68f26286b6a3f1166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc725d3945514e699a87ecc5b88370cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6228f46dba4febabbea8e96bfd7bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b169fa52704163b9ae7bf43c58ccf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97412157b14c48ae89c2ace9f007e29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9647f1d40fc4cc09cae080af44cd743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd52a0a9b8394f6ca0e8a79cdecd1f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "164294e677204a5fa6e85fa4034d8469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14323bb6a2574eccb4ba6b7e49bbfea5",
              "IPY_MODEL_2e34720695734be3afc83a584c423548",
              "IPY_MODEL_550f847c918b4973abc0a8f7e8a87384"
            ],
            "layout": "IPY_MODEL_ed21bae00cab4eb0a70b2a93459bf9dd"
          }
        },
        "14323bb6a2574eccb4ba6b7e49bbfea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fefce3b40bbb4a058406fb1ea9d40ae3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f78bbeae508242b6bef841ef3f8d36ee",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "2e34720695734be3afc83a584c423548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0863b8914dc54b6fbd07fb831de90333",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af05b57838894357a8ae9cee8a1e42f0",
            "value": 124
          }
        },
        "550f847c918b4973abc0a8f7e8a87384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd29ab040d51413aa5b0b1dd932d971f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6051186e33b0427584072ffa66be58e1",
            "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá8.46kB/s]"
          }
        },
        "ed21bae00cab4eb0a70b2a93459bf9dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fefce3b40bbb4a058406fb1ea9d40ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78bbeae508242b6bef841ef3f8d36ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0863b8914dc54b6fbd07fb831de90333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af05b57838894357a8ae9cee8a1e42f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd29ab040d51413aa5b0b1dd932d971f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6051186e33b0427584072ffa66be58e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}