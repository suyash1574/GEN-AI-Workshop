{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyash1574/GEN-AI-Workshop/blob/main/src/day1/notebooks/03_text_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lehT7-vBigZb"
      },
      "source": [
        "# Day 1: Text Pipeline - Your First Language Model\n",
        "\n",
        "Welcome to hands-on text processing! Now that you understand neural networks, let's explore how they work with text data.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "- Understand how text becomes numbers (tokenization)\n",
        "- Load and use a pre-trained language model\n",
        "- Experiment with text generation parameters\n",
        "- Compare different prompt engineering techniques\n",
        "- Build your first text generation pipeline\n",
        "\n",
        "## üìö Research Focus\n",
        "This notebook emphasizes **discovery learning**. You'll:\n",
        "1. Research concepts before implementing\n",
        "2. Experiment with parameters to see their effects\n",
        "3. Compare different approaches\n",
        "4. Build understanding through hands-on exploration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T66KHTnGigZg"
      },
      "source": [
        "## 1. From Text to Numbers\n",
        "\n",
        "Neural networks work with numbers, but we have text. How do we bridge this gap?\n",
        "\n",
        "üîç **RESEARCH TASK 1**:\n",
        "- What is tokenization in NLP?\n",
        "- What is the difference between word-level and sub-word tokenization?\n",
        "- Research \"BPE\" (Byte Pair Encoding) - how does it work?\n",
        "- Why can't we just assign each word a number?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wzgU0amLigZh",
        "outputId": "05d990da-5445-48f8-edeb-7b816814d8e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XHZYarpigZj"
      },
      "source": [
        "### Exploring Tokenization\n",
        "\n",
        "üîç **RESEARCH TASK 2**:\n",
        "- Look up the GPT-2 tokenizer documentation\n",
        "- What is a \"vocabulary size\"?\n",
        "- What happens when the model encounters a word it's never seen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-okZ4tETigZk"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the GPT-2 tokenizer\n",
        "# Hint: Use GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Test sentences to explore tokenization\n",
        "test_sentences = [\n",
        "    \"Hello world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is revolutionizing technology.\",\n",
        "    \"GPT-2 uses transformer architecture.\",\n",
        "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
        "]\n",
        "\n",
        "print(\"üîç Exploring Tokenization:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # TODO: Tokenize the sentence\n",
        "    # Hint: Use tokenizer.encode() to get token IDs\n",
        "    # Use tokenizer.tokenize() to see the actual tokens\n",
        "    tokens = ____  # Get the actual token strings\n",
        "    token_ids = ____  # Get the numerical IDs\n",
        "\n",
        "    print(f\"\\nOriginal: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# TODO: Print tokenizer vocabulary size\n",
        "print(f\"\\nüìä Tokenizer vocabulary size: {len(tokenizer)}\")  # Hint: len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKXGzFcigZl"
      },
      "source": [
        "### Understanding Token Patterns\n",
        "\n",
        "üîç **RESEARCH TASK 3**:\n",
        "- Why do some words get split into multiple tokens?\n",
        "- What does the 'ƒ†' symbol represent in GPT-2 tokens?\n",
        "- How might tokenization affect model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAjiAzvxigZl"
      },
      "outputs": [],
      "source": [
        "# Analyze tokenization patterns\n",
        "analysis_texts = [\n",
        "    \"running\",\n",
        "    \"runner\",\n",
        "    \"run\",\n",
        "    \"unhappiness\",\n",
        "    \"ChatGPT\",\n",
        "    \"COVID-19\",\n",
        "    \"2023\",\n",
        "    \"programming\",\n",
        "    \"antidisestablishmentarianism\"\n",
        "]\n",
        "\n",
        "print(\"üîç Token Pattern Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "token_analysis = []\n",
        "\n",
        "for text in analysis_texts:\n",
        "    # TODO: Analyze each text\n",
        "    tokens = ____  # Tokenize the text\n",
        "    token_count = ____  # Count the tokens\n",
        "\n",
        "    token_analysis.append({\n",
        "        'text': text,\n",
        "        'tokens': tokens,\n",
        "        'token_count': token_count,\n",
        "        'chars_per_token': len(text) / token_count\n",
        "    })\n",
        "\n",
        "    print(f\"{text:30} ‚Üí {tokens} ({token_count} tokens)\")\n",
        "\n",
        "# TODO: Create a DataFrame and analyze patterns\n",
        "df = pd.DataFrame(token_analysis)\n",
        "print(f\"\\nüìä Average characters per token: {____:.2f}\")  # Calculate mean\n",
        "print(f\"üìä Longest word in tokens: {____}\")  # Find max token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsvgPrQiigZl"
      },
      "source": [
        "## 2. Loading Your First Language Model\n",
        "\n",
        "Now let's load GPT-2 and understand its architecture.\n",
        "\n",
        "üîç **RESEARCH TASK 4**:\n",
        "- What is GPT-2 and when was it released?\n",
        "- How many parameters does GPT-2 have? (Compare different sizes)\n",
        "- What is \"autoregressive\" text generation?\n",
        "- How does GPT-2 relate to the neural network you built in the previous notebook?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRQI8fwHigZm"
      },
      "outputs": [],
      "source": [
        "# TODO: Load GPT-2 model\n",
        "# Hint: Use GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"üîÑ Loading GPT-2 model (this may take a moment)...\")\n",
        "model = ____\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: Use model.eval()\n",
        "____\n",
        "\n",
        "print(\"‚úÖ GPT-2 model loaded successfully!\")\n",
        "\n",
        "# Explore model architecture\n",
        "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "\n",
        "# TODO: Count model parameters\n",
        "# Hint: sum(p.numel() for p in model.parameters())\n",
        "total_params = ____\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSDWHntfigZm"
      },
      "source": [
        "### Understanding Model Architecture\n",
        "\n",
        "üîç **RESEARCH TASK 5**:\n",
        "- What are \"transformer blocks\" in GPT-2?\n",
        "- What is \"attention\" in the context of neural networks?\n",
        "- How does this compare to the simple network you built earlier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6aIdJfBigZm"
      },
      "outputs": [],
      "source": [
        "# Explore model structure\n",
        "print(\"üîç Model Structure Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Print model configuration\n",
        "# Hint: Use model.config\n",
        "config = ____\n",
        "\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Maximum sequence length: {config.n_positions}\")\n",
        "print(f\"Number of transformer layers: {config.n_layer}\")\n",
        "print(f\"Number of attention heads: {config.n_head}\")\n",
        "print(f\"Hidden size: {config.n_embd}\")\n",
        "\n",
        "# Compare to your simple network\n",
        "print(\"\\nü§î Comparison to Your Neural Network:\")\n",
        "print(f\"Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\")\n",
        "print(f\"GPT-2 has: {config.vocab_size} inputs ‚Üí {config.n_embd} hidden ‚Üí {config.vocab_size} outputs\")\n",
        "print(f\"Your network: ~50 parameters\")\n",
        "print(f\"GPT-2: {total_params:,} parameters\")\n",
        "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSsrEUG_igZn"
      },
      "source": [
        "## 3. Text Generation Experiments\n",
        "\n",
        "Let's generate text and understand how different parameters affect the output.\n",
        "\n",
        "üîç **RESEARCH TASK 6**:\n",
        "- What is \"temperature\" in text generation?\n",
        "- What is \"top-p\" (nucleus) sampling?\n",
        "- What's the difference between greedy decoding and sampling?\n",
        "- How do these parameters affect creativity vs. coherence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5YoE1J6igZn"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a text generation pipeline\n",
        "# Hint: Use pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "generator = ____\n",
        "\n",
        "# Base prompt for experiments\n",
        "base_prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "print(f\"ü§ñ Base prompt: '{base_prompt}'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHh8t9CigZn"
      },
      "source": [
        "### Temperature Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 7**:\n",
        "- What happens when temperature = 0?\n",
        "- What happens when temperature > 1?\n",
        "- Why might you want different temperatures for different tasks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRz5quFoigZn"
      },
      "outputs": [],
      "source": [
        "# Experiment with different temperatures\n",
        "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
        "\n",
        "print(\"üå°Ô∏è Temperature Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nüî• Temperature: {temp}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different temperatures\n",
        "    # Hint: Use generator() with temperature parameter\n",
        "    result = generator(\n",
        "        ____,  # prompt\n",
        "        max_length=____,  # try 60\n",
        "        temperature=____,  # use the temp variable\n",
        "        do_sample=____,  # should be True for sampling\n",
        "        pad_token_id=____  # use tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # TODO: Print the generated text\n",
        "    generated_text = ____  # Extract from result\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ Which temperature produced the most coherent text?\")\n",
        "print(\"‚Ä¢ Which was most creative/surprising?\")\n",
        "print(\"‚Ä¢ When might you use each temperature setting?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PGFsaXDigZo"
      },
      "source": [
        "### Top-p (Nucleus) Sampling Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 8**:\n",
        "- How does top-p sampling work?\n",
        "- What's the difference between top-k and top-p sampling?\n",
        "- Why might top-p be better than just using temperature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ7c1yiBigZo"
      },
      "outputs": [],
      "source": [
        "# Experiment with top-p sampling\n",
        "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
        "\n",
        "print(\"üéØ Top-p Sampling Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for top_p in top_p_values:\n",
        "    print(f\"\\nüé≤ Top-p: {top_p}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different top-p values\n",
        "    result = generator(\n",
        "        base_prompt,\n",
        "        max_length=60,\n",
        "        temperature=0.8,  # Keep temperature constant\n",
        "        top_p=____,  # Use the top_p variable\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ How did the outputs change with different top-p values?\")\n",
        "print(\"‚Ä¢ What's the trade-off between diversity and quality?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXLK5NpigZo"
      },
      "source": [
        "## 4. Prompt Engineering Experiments\n",
        "\n",
        "The way you phrase your prompt dramatically affects the output.\n",
        "\n",
        "üîç **RESEARCH TASK 9**:\n",
        "- What is \"prompt engineering\"?\n",
        "- What are \"few-shot\" prompts?\n",
        "- How can prompt structure influence model behavior?\n",
        "- Research common prompt engineering techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnERVPBEigZo"
      },
      "outputs": [],
      "source": [
        "# Different prompt styles to experiment with\n",
        "prompts_to_test = {\n",
        "    \"Direct\": \"Write about artificial intelligence:\",\n",
        "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
        "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
        "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
        "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
        "    \"Few_Shot\": \"Technology predictions:\\n‚Ä¢ The internet will connect everyone (1990s)\\n‚Ä¢ Smartphones will be everywhere (2000s)\\n‚Ä¢ Artificial intelligence will\"\n",
        "}\n",
        "\n",
        "print(\"‚úçÔ∏è Prompt Engineering Experiments:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# TODO: Test each prompt style\n",
        "for style, prompt in prompts_to_test.items():\n",
        "    print(f\"\\nüìù Style: {style}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate text for each prompt\n",
        "    result = generator(\n",
        "        ____,  # use the prompt variable\n",
        "        max_length=80,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQkexDsBigZo"
      },
      "source": [
        "### Analyzing Prompt Effectiveness\n",
        "\n",
        "üîç **RESEARCH TASK 10**:\n",
        "- Which prompt style produced the most useful output?\n",
        "- How did the model's \"behavior\" change with different prompts?\n",
        "- What makes a good prompt?\n",
        "- How might this apply to chatbots or AI assistants?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8brmkwsigZp"
      },
      "outputs": [],
      "source": [
        "# Let's analyze the generated text more systematically\n",
        "print(\"üìä Prompt Analysis Exercise:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: For each prompt style, generate multiple outputs and analyze\n",
        "analysis_results = []\n",
        "\n",
        "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
        "    # Generate 3 outputs for each prompt\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        # TODO: Generate text\n",
        "        result = generator(\n",
        "            prompt,\n",
        "            max_length=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        output = result[0]['generated_text']\n",
        "        outputs.append(output)\n",
        "\n",
        "    # TODO: Analyze the outputs\n",
        "    lengths = ___\n",
        "    avg_length = ___ # Calculate average length of outputs\n",
        "\n",
        "    analysis_results.append({\n",
        "        'style': style,\n",
        "        'prompt': prompt,\n",
        "        'avg_length': avg_length,\n",
        "        'outputs': outputs\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{style}:\")\n",
        "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
        "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
        "\n",
        "print(\"\\nü§î Reflection Questions:\")\n",
        "print(\"‚Ä¢ Which prompt style was most consistent?\")\n",
        "print(\"‚Ä¢ Which produced the most relevant outputs?\")\n",
        "print(\"‚Ä¢ How might you improve these prompts?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvObchkgigZp"
      },
      "source": [
        "## 5. Building Your Text Generation Pipeline\n",
        "\n",
        "Now let's create a customizable text generation function.\n",
        "\n",
        "üîç **RESEARCH TASK 11**:\n",
        "- What parameters should a good text generation function have?\n",
        "- How can you make text generation more controllable?\n",
        "- What are the trade-offs between different generation strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SO8TzznigZq"
      },
      "outputs": [],
      "source": [
        "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
        "    \"\"\"\n",
        "    TODO: Create a customizable text generation function\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt\n",
        "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
        "        length (str): \"short\", \"medium\", or \"long\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Set parameters based on style\n",
        "    if style == \"creative\":\n",
        "        temperature = ____  # Higher for creativity\n",
        "        top_p = ____        # Higher for diversity\n",
        "    elif style == \"conservative\":\n",
        "        temperature = ____  # Lower for consistency\n",
        "        top_p = ____        # Lower for focus\n",
        "    else:  # balanced\n",
        "        temperature = ____  # Medium values\n",
        "        top_p = ____\n",
        "\n",
        "    # TODO: Set length based on parameter\n",
        "    if length == \"short\":\n",
        "        max_length = ____  # Try 40\n",
        "    elif length == \"long\":\n",
        "        max_length = ____  # Try 100\n",
        "    else:  # medium\n",
        "        max_length = ____  # Try 70\n",
        "\n",
        "    # TODO: Generate text with the parameters\n",
        "    result = generator(\n",
        "        ____,  # prompt\n",
        "        max_length=____,\n",
        "        temperature=____,\n",
        "        top_p=____,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Test your function\n",
        "test_prompt = \"The future of education will be\"\n",
        "\n",
        "print(\"üß™ Testing Your Text Generator:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Test different combinations\n",
        "test_combinations = [\n",
        "    (\"creative\", \"short\"),\n",
        "    (\"balanced\", \"medium\"),\n",
        "    (\"conservative\", \"long\")\n",
        "]\n",
        "\n",
        "for style, length in test_combinations:\n",
        "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Use your function\n",
        "    output = custom_text_generator(____)\n",
        "    print(output)\n",
        "    print(f\"Characters: {len(output)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVt6HX5figZq"
      },
      "source": [
        "## 6. Creative Applications\n",
        "\n",
        "Let's explore some creative uses of text generation.\n",
        "\n",
        "üîç **RESEARCH TASK 12**:\n",
        "- How is GPT-2 being used in creative writing?\n",
        "- What are some potential applications for businesses?\n",
        "- What ethical considerations should we keep in mind?\n",
        "- How might this technology evolve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Ouhfj4igZq"
      },
      "outputs": [],
      "source": [
        "# Creative applications to try\n",
        "creative_prompts = {\n",
        "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
        "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
        "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
        "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
        "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
        "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
        "}\n",
        "\n",
        "print(\"üé® Creative Applications:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Generate creative content\n",
        "for app_type, prompt in creative_prompts.items():\n",
        "    print(f\"\\nüñºÔ∏è {app_type}:\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Choose appropriate style for each application\n",
        "    if app_type in [\"Poetry\", \"Story\"]:\n",
        "        style = ____  # Should be creative\n",
        "    elif app_type in [\"Product Description\", \"Email\"]:\n",
        "        style = ____  # Should be conservative\n",
        "    else:\n",
        "        style = ____  # Should be balanced\n",
        "\n",
        "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
        "    print(output)\n",
        "    print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3MaAlsJigZq"
      },
      "source": [
        "## 7. Understanding Limitations\n",
        "\n",
        "It's important to understand what language models can and cannot do.\n",
        "\n",
        "üîç **RESEARCH TASK 13**:\n",
        "- What is \"hallucination\" in language models?\n",
        "- Why might GPT-2 generate biased or incorrect information?\n",
        "- What are the limitations of autoregressive generation?\n",
        "- How do these limitations affect real-world applications?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXELGphPigZq"
      },
      "outputs": [],
      "source": [
        "# Test model limitations\n",
        "limitation_tests = {\n",
        "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
        "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
        "    \"Math\": \"What is 47 * 83? The answer is\",\n",
        "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
        "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
        "}\n",
        "\n",
        "print(\"‚ö†Ô∏è Understanding Model Limitations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for test_type, prompt in limitation_tests.items():\n",
        "    print(f\"\\nüß™ Testing: {test_type}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate responses to test limitations\n",
        "    output = custom_text_generator(\n",
        "        ____,  # prompt\n",
        "        style=\"conservative\",  # Use conservative for factual tasks\n",
        "        length=\"short\"\n",
        "    )\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    # TODO: Analyze the output\n",
        "    print(f\"ü§î Analysis: Does this look correct/reasonable?\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Important Reminders:\")\n",
        "print(\"‚Ä¢ Language models can generate plausible-sounding but incorrect information\")\n",
        "print(\"‚Ä¢ Always verify factual claims from AI-generated content\")\n",
        "print(\"‚Ä¢ Be aware of potential biases in training data\")\n",
        "print(\"‚Ä¢ Use AI as a tool to assist, not replace, human judgment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axckvie5igZr"
      },
      "source": [
        "## 8. Reflection and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "‚úÖ **Understood tokenization and text preprocessing**\n",
        "‚úÖ **Loaded and used a pre-trained language model**\n",
        "‚úÖ **Experimented with generation parameters**\n",
        "‚úÖ **Explored prompt engineering techniques**\n",
        "‚úÖ **Built a customizable text generation pipeline**\n",
        "‚úÖ **Understood model limitations and ethical considerations**\n",
        "\n",
        "### Key Insights\n",
        "üîç **Discussion Questions**:\n",
        "- What surprised you most about text generation?\n",
        "- Which prompt engineering technique was most effective?\n",
        "- How might you use this in a real project?\n",
        "- What limitations concerned you most?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxIQBWtJigZr"
      },
      "outputs": [],
      "source": [
        "# Final experiment: Design your own use case\n",
        "print(\"üéØ FINAL CHALLENGE:\")\n",
        "print(\"Design your own text generation use case!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Create your own application\n",
        "# Ideas: Story generator, email assistant, creative writing helper, etc.\n",
        "\n",
        "your_use_case = \"____\"  # Describe your use case\n",
        "your_prompt = \"____\"   # Design your prompt\n",
        "your_style = \"____\"    # Choose your style\n",
        "your_length = \"____\"   # Choose your length\n",
        "\n",
        "print(f\"üìù Your use case: {your_use_case}\")\n",
        "print(f\"üìù Your prompt: '{your_prompt}'\")\n",
        "print(f\"üìù Your settings: {your_style}, {your_length}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# TODO: Generate with your custom settings\n",
        "your_output = custom_text_generator(____)\n",
        "print(\"üéâ Your generated content:\")\n",
        "print(your_output)\n",
        "\n",
        "print(\"\\nüìà Next Steps:\")\n",
        "print(\"‚Ä¢ Experiment with different prompt formats\")\n",
        "print(\"‚Ä¢ Try combining multiple generation calls\")\n",
        "print(\"‚Ä¢ Think about how to validate or improve outputs\")\n",
        "print(\"‚Ä¢ Consider user interface design for your application\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loqO5x9MigZr"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "- ‚úÖ Mastered text tokenization and preprocessing\n",
        "- ‚úÖ Used a state-of-the-art language model\n",
        "- ‚úÖ Discovered the art and science of prompt engineering\n",
        "- ‚úÖ Built your own text generation pipeline\n",
        "- ‚úÖ Understood the capabilities and limitations of AI text generation\n",
        "- ‚úÖ Explored creative applications\n",
        "\n",
        "### Prepare for the Next Notebook\n",
        "Next, we'll explore computer vision and image processing, applying similar principles to visual data!\n",
        "\n",
        "**Share with your partner**: What was your most successful text generation experiment?\n",
        "\n",
        "---\n",
        "*Text Pipeline Complete - Ready for Computer Vision! üñºÔ∏è*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}