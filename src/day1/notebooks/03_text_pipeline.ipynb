{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyash1574/GEN-AI-Workshop/blob/main/src/day1/notebooks/03_text_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lehT7-vBigZb"
      },
      "source": [
        "# Day 1: Text Pipeline - Your First Language Model\n",
        "\n",
        "Welcome to hands-on text processing! Now that you understand neural networks, let's explore how they work with text data.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "- Understand how text becomes numbers (tokenization)\n",
        "- Load and use a pre-trained language model\n",
        "- Experiment with text generation parameters\n",
        "- Compare different prompt engineering techniques\n",
        "- Build your first text generation pipeline\n",
        "\n",
        "## üìö Research Focus\n",
        "This notebook emphasizes **discovery learning**. You'll:\n",
        "1. Research concepts before implementing\n",
        "2. Experiment with parameters to see their effects\n",
        "3. Compare different approaches\n",
        "4. Build understanding through hands-on exploration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T66KHTnGigZg"
      },
      "source": [
        "## 1. From Text to Numbers\n",
        "\n",
        "Neural networks work with numbers, but we have text. How do we bridge this gap?\n",
        "\n",
        "üîç **RESEARCH TASK 1**:\n",
        "- What is tokenization in NLP?\n",
        "- What is the difference between word-level and sub-word tokenization?\n",
        "- Research \"BPE\" (Byte Pair Encoding) - how does it work?\n",
        "- Why can't we just assign each word a number?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzgU0amLigZh",
        "outputId": "9c93cfda-3e7a-44ec-8fd1-1a0a3bd41997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XHZYarpigZj"
      },
      "source": [
        "### Exploring Tokenization\n",
        "\n",
        "üîç **RESEARCH TASK 2**:\n",
        "- Look up the GPT-2 tokenizer documentation\n",
        "- What is a \"vocabulary size\"?\n",
        "- What happens when the model encounters a word it's never seen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-okZ4tETigZk",
        "outputId": "2ee86ed6-5308-4016-e594-ee03c2c0547e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Exploring Tokenization:\n",
            "==================================================\n",
            "\n",
            "Original: Hello world!\n",
            "Tokens: ['Hello', 'ƒ†world', '!']\n",
            "Token IDs: [15496, 995, 0]\n",
            "Number of tokens: 3\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Tokens: ['The', 'ƒ†quick', 'ƒ†brown', 'ƒ†fox', 'ƒ†jumps', 'ƒ†over', 'ƒ†the', 'ƒ†lazy', 'ƒ†dog', '.']\n",
            "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Number of tokens: 10\n",
            "\n",
            "Original: Artificial intelligence is revolutionizing technology.\n",
            "Tokens: ['Art', 'ificial', 'ƒ†intelligence', 'ƒ†is', 'ƒ†revolution', 'izing', 'ƒ†technology', '.']\n",
            "Token IDs: [8001, 9542, 4430, 318, 5854, 2890, 3037, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: GPT-2 uses transformer architecture.\n",
            "Tokens: ['G', 'PT', '-', '2', 'ƒ†uses', 'ƒ†transformer', 'ƒ†architecture', '.']\n",
            "Token IDs: [38, 11571, 12, 17, 3544, 47385, 10959, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: Supercalifragilisticexpialidocious\n",
            "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
            "Token IDs: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
            "Number of tokens: 11\n",
            "\n",
            "üìä Tokenizer vocabulary size: 50257\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the GPT-2 tokenizer\n",
        "# Hint: Use GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Test sentences to explore tokenization\n",
        "test_sentences = [\n",
        "    \"Hello world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is revolutionizing technology.\",\n",
        "    \"GPT-2 uses transformer architecture.\",\n",
        "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
        "]\n",
        "\n",
        "print(\"üîç Exploring Tokenization:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # TODO: Tokenize the sentence\n",
        "    # Hint: Use tokenizer.encode() to get token IDs\n",
        "    # Use tokenizer.tokenize() to see the actual tokens\n",
        "    tokens = tokenizer.tokenize(sentence)  # Get the actual token strings\n",
        "    token_ids = tokenizer.encode(sentence)  # Get the numerical IDs\n",
        "\n",
        "    print(f\"\\nOriginal: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# TODO: Print tokenizer vocabulary size\n",
        "print(f\"\\nüìä Tokenizer vocabulary size: {len(tokenizer)}\")  # Hint: len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKXGzFcigZl"
      },
      "source": [
        "### Understanding Token Patterns\n",
        "\n",
        "üîç **RESEARCH TASK 3**:\n",
        "- Why do some words get split into multiple tokens?\n",
        "- What does the 'ƒ†' symbol represent in GPT-2 tokens?\n",
        "- How might tokenization affect model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAjiAzvxigZl",
        "outputId": "95da3d8d-9538-4715-b18d-d8b36ac27dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Token Pattern Analysis:\n",
            "============================================================\n",
            "running                        ‚Üí ['running'] (1 tokens)\n",
            "runner                         ‚Üí ['runner'] (1 tokens)\n",
            "run                            ‚Üí ['run'] (1 tokens)\n",
            "unhappiness                    ‚Üí ['un', 'h', 'appiness'] (3 tokens)\n",
            "ChatGPT                        ‚Üí ['Chat', 'G', 'PT'] (3 tokens)\n",
            "COVID-19                       ‚Üí ['CO', 'VID', '-', '19'] (4 tokens)\n",
            "2023                           ‚Üí ['20', '23'] (2 tokens)\n",
            "programming                    ‚Üí ['program', 'ming'] (2 tokens)\n",
            "antidisestablishmentarianism   ‚Üí ['ant', 'idis', 'establishment', 'arian', 'ism'] (5 tokens)\n",
            "\n",
            "üìä Average characters per token: 4.12\n",
            "üìä Longest word in tokens: antidisestablishmentarianism\n"
          ]
        }
      ],
      "source": [
        "# Analyze tokenization patterns\n",
        "analysis_texts = [\n",
        "    \"running\",\n",
        "    \"runner\",\n",
        "    \"run\",\n",
        "    \"unhappiness\",\n",
        "    \"ChatGPT\",\n",
        "    \"COVID-19\",\n",
        "    \"2023\",\n",
        "    \"programming\",\n",
        "    \"antidisestablishmentarianism\"\n",
        "]\n",
        "\n",
        "print(\"üîç Token Pattern Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "token_analysis = []\n",
        "\n",
        "for text in analysis_texts:\n",
        "    # TODO: Analyze each text\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
        "    token_ids = tokenizer.encode(text)  # Get token IDs\n",
        "    token_count = len(token_ids) # Count the tokens\n",
        "\n",
        "\n",
        "    token_analysis.append({\n",
        "        'text': text,\n",
        "        'tokens': tokens,\n",
        "        'token_count': token_count,\n",
        "        'chars_per_token': len(text) / token_count if token_count > 0 else 0 # Avoid division by zero\n",
        "    })\n",
        "\n",
        "    print(f\"{text:30} ‚Üí {tokens} ({token_count} tokens)\")\n",
        "\n",
        "# TODO: Create a DataFrame and analyze patterns\n",
        "df = pd.DataFrame(token_analysis)\n",
        "print(f\"\\nüìä Average characters per token: {df['chars_per_token'].mean():.2f}\")  # Calculate mean\n",
        "print(f\"üìä Longest word in tokens: {df.loc[df['token_count'].idxmax()]['text']}\")  # Find max token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsvgPrQiigZl"
      },
      "source": [
        "## 2. Loading Your First Language Model\n",
        "\n",
        "Now let's load GPT-2 and understand its architecture.\n",
        "\n",
        "üîç **RESEARCH TASK 4**:\n",
        "- What is GPT-2 and when was it released?\n",
        "- How many parameters does GPT-2 have? (Compare different sizes)\n",
        "- What is \"autoregressive\" text generation?\n",
        "- How does GPT-2 relate to the neural network you built in the previous notebook?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRQI8fwHigZm",
        "outputId": "f3ce666a-c80f-4557-d1cd-4246495f347f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GPT-2 model (this may take a moment)...\n",
            "‚úÖ GPT-2 model loaded successfully!\n",
            "\n",
            "üèóÔ∏è Model Architecture:\n",
            "Model type: GPT2LMHeadModel\n",
            "Total parameters: 124,439,808\n",
            "Model size: ~124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load GPT-2 model\n",
        "# Hint: Use GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"üîÑ Loading GPT-2 model (this may take a moment)...\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: Use model.eval()\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ GPT-2 model loaded successfully!\")\n",
        "\n",
        "# Explore model architecture\n",
        "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "\n",
        "# TODO: Count model parameters\n",
        "# Hint: sum(p.numel() for p in model.parameters())\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSDWHntfigZm"
      },
      "source": [
        "### Understanding Model Architecture\n",
        "\n",
        "üîç **RESEARCH TASK 5**:\n",
        "- What are \"transformer blocks\" in GPT-2?\n",
        "- What is \"attention\" in the context of neural networks?\n",
        "- How does this compare to the simple network you built earlier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6aIdJfBigZm",
        "outputId": "afdeaf10-3e28-40d3-dfe8-a6ee76a68d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Model Structure Analysis:\n",
            "==================================================\n",
            "Vocabulary size: 50257\n",
            "Maximum sequence length: 1024\n",
            "Number of transformer layers: 12\n",
            "Number of attention heads: 12\n",
            "Hidden size: 768\n",
            "\n",
            "ü§î Comparison to Your Neural Network:\n",
            "Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\n",
            "GPT-2 has: 50257 inputs ‚Üí 768 hidden ‚Üí 50257 outputs\n",
            "Your network: ~50 parameters\n",
            "GPT-2: 124,439,808 parameters\n",
            "GPT-2 is ~2,488,796x larger!\n"
          ]
        }
      ],
      "source": [
        "# Explore model structure\n",
        "print(\"üîç Model Structure Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Print model configuration\n",
        "# Hint: Use model.config\n",
        "config = model.config\n",
        "\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Maximum sequence length: {config.n_positions}\")\n",
        "print(f\"Number of transformer layers: {config.n_layer}\")\n",
        "print(f\"Number of attention heads: {config.n_head}\")\n",
        "print(f\"Hidden size: {config.n_embd}\")\n",
        "\n",
        "# Compare to your simple network\n",
        "print(\"\\nü§î Comparison to Your Neural Network:\")\n",
        "print(f\"Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\")\n",
        "print(f\"GPT-2 has: {config.vocab_size} inputs ‚Üí {config.n_embd} hidden ‚Üí {config.vocab_size} outputs\")\n",
        "print(f\"Your network: ~50 parameters\")\n",
        "print(f\"GPT-2: {total_params:,} parameters\")\n",
        "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSsrEUG_igZn"
      },
      "source": [
        "## 3. Text Generation Experiments\n",
        "\n",
        "Let's generate text and understand how different parameters affect the output.\n",
        "\n",
        "üîç **RESEARCH TASK 6**:\n",
        "- What is \"temperature\" in text generation?\n",
        "- What is \"top-p\" (nucleus) sampling?\n",
        "- What's the difference between greedy decoding and sampling?\n",
        "- How do these parameters affect creativity vs. coherence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5YoE1J6igZn",
        "outputId": "38db8bd5-46a6-4472-ab85-f74e4cd1e5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Base prompt: 'In the future, artificial intelligence will'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a text generation pipeline\n",
        "# Hint: Use pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "generator =pipeline('text-generation' , model=model , tokenizer=tokenizer)\n",
        "\n",
        "# Base prompt for experiments\n",
        "base_prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "print(f\"ü§ñ Base prompt: '{base_prompt}'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHh8t9CigZn"
      },
      "source": [
        "### Temperature Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 7**:\n",
        "- What happens when temperature = 0?\n",
        "- What happens when temperature > 1?\n",
        "- Why might you want different temperatures for different tasks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRz5quFoigZn",
        "outputId": "0f5ffa03-9671-4931-fd57-82a54d545751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå°Ô∏è Temperature Experiments:\n",
            "==================================================\n",
            "\n",
            "üî• Temperature: 0.1\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more complex than we've ever imagined.\n",
            "\n",
            "The future of AI is going to be a lot more\n",
            "\n",
            "üî• Temperature: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will do a lot more than just figure out the right way to handle the problem of what needs to be solved. That's why we need to invest in artificial intelligence so that we can find the best ways to solve the problem of what needs to be solved.\n",
            "\n",
            "How will AI work with human intelligence?\n",
            "\n",
            "The AI will play a critical role in this. The AI will be able to do the work needed to solve a problem. The AI will help with the processes that are needed to do the work needed to solve a problem.\n",
            "\n",
            "The AI will be able to solve a problem in a way that is not constrained by human limitations. It will be able to solve a problem in a way that is not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits. It will be able to solve a problem that is not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits.\n",
            "\n",
            "So AI will be able to solve problems that are not constrained by human limitations. It will be able to solve problems that are not constrained by human limitations. It will be able to solve a problem that is not constrained by human limits. It will be able to solve a problem that is\n",
            "\n",
            "üî• Temperature: 1.0\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will grow to be powerful, but also scalable to every sector of the military and government.\n",
            "\n",
            "The U.S. believes that the world will soon come together to make sure the next generation of computers does indeed be ready today so that it can do everything necessary to become better at tasks such as intelligence, transportation, financial, communications technologies and energy.\n",
            "\n",
            "But for now, the world will be left with artificial intelligence. What kind of information can you use to do good things today, or to do better tomorrow?\n",
            "\n",
            "In November 2010, the U.S. General Services Administration asked the Defense Energy Research and Development Agency to ask for a $3.16 billion contract for a new U.S. military industrial base in the Indian Ocean.\n",
            "\n",
            "It has not been officially approved. But the U.S. government has long been known for getting things done with computers. The government even allowed the U.S. Navy to put computers in naval vessels. Those efforts have now been shut down amid delays of orders from the Obama administration to open the C-130 cargo ship to foreign customers.\n",
            "\n",
            "A year and half ago, the Department of Transportation gave two government agencies permission, the National Transportation Safety Board (NTSB), to study the impact of U\n",
            "\n",
            "üî• Temperature: 1.5\n",
            "------------------------------\n",
            "In the future, artificial intelligence will help build tools, services and applications to do that while maintaining the user experience. The goal of \"computing intelligent assistants\" will also vary depending on many different industries and industries. In this post we outline all that we'll focus on since our research suggests that AI algorithms to understand a person (computer scientist) might or might not be all that common in recent times. Please note, these recommendations assume at least some experience at least one job. The most common ones cited were those who held a PhD in biological (science, surgery, philosophy or health sciences and some business or business investment management positions including insurance)\n",
            "\n",
            "Research shows many types of computing, but mostly computing in the data and analysis arena\n",
            "\n",
            "Some scientists even have specialized areas of expertise that AI might work as long as there was little or no chance that AI would change in order to create an efficient machine\n",
            "\n",
            "\n",
            "These skills would not be common by now because people have already learned enough how AI-powered technologies can improve. However, many scientists say we are all beginning to understand how to implement \"computing intelligently\", so it seems worth going through these recommendations in full and looking.\n",
            "\n",
            "These algorithms should support the following things‚Ä¶\n",
            "\n",
            "What you do with AI\n",
            "\n",
            "\n",
            "This may not include using machine\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ Which temperature produced the most coherent text?\n",
            "‚Ä¢ Which was most creative/surprising?\n",
            "‚Ä¢ When might you use each temperature setting?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different temperatures\n",
        "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
        "\n",
        "print(\"üå°Ô∏è Temperature Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nüî• Temperature: {temp}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different temperatures\n",
        "    # Hint: Use generator() with temperature parameter\n",
        "    result = generator(\n",
        "        base_prompt,  # prompt\n",
        "        max_length=60,  # try 60\n",
        "        temperature=temp,  # use the temp variable\n",
        "        do_sample=True,  # should be True for sampling\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # TODO: Print the generated text\n",
        "    generated_text = result[0]['generated_text']  # Extract from result\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ Which temperature produced the most coherent text?\")\n",
        "print(\"‚Ä¢ Which was most creative/surprising?\")\n",
        "print(\"‚Ä¢ When might you use each temperature setting?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PGFsaXDigZo"
      },
      "source": [
        "### Top-p (Nucleus) Sampling Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 8**:\n",
        "- How does top-p sampling work?\n",
        "- What's the difference between top-k and top-p sampling?\n",
        "- Why might top-p be better than just using temperature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ7c1yiBigZo",
        "outputId": "8273d177-e1c9-40d1-d7d2-2b283600bad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Top-p Sampling Experiments:\n",
            "==================================================\n",
            "\n",
            "üé≤ Top-p: 0.3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to help us better understand our own lives, and to help us better understand others.\n",
            "\n",
            "The future of AI is a very exciting one. It is a new era in which we are seeing the emergence of new technologies that will change the way we think, act, and think.\n",
            "\n",
            "The future of AI is a very exciting one. It is a new era in which we are seeing the emergence of new technologies that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act, and think.\n",
            "\n",
            "We are in a new era of technology that will change the way we think, act\n",
            "\n",
            "üé≤ Top-p: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to be used to solve problems of complexity, intelligence, and intelligence-based solutions.\n",
            "\n",
            "The \"Cognitive Science of Artificial Intelligence\" (CSI) is a research and development project that aims to develop the ability to design, build, and deploy cognitive systems. CSI will be led by the AI Lab at the University of Toronto. The project is being led by the AI Lab at the University of Toronto.\n",
            "\n",
            "The CSI will focus on understanding how the human brain works, how it operates, and how it interacts with the environment. It will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial Intelligence (ICAI) in Barcelona, Spain, on June 15-16, 2017.\n",
            "\n",
            "The CSI will be presented at the IEEE International Conference on Artificial\n",
            "\n",
            "üé≤ Top-p: 0.9\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to work with many of the same features as humans, including spatial navigation and the ability to control computers.\n",
            "\n",
            "The team has also explored the possibility of using AI for scientific research, which will include the creation of intelligent robots and robots that would be capable of performing tasks that humans could not.\n",
            "\n",
            "\"We are also interested in using robots to help people understand their environment, to help them develop new skills, to create new products, and to find new solutions to our problems,\" said Paul Shultz, head of AI at Google. \"This is one of the most exciting opportunities for humans to explore.\"\n",
            "\n",
            "In the meantime, the team plans to continue to develop its AI projects. In the future, they will be able to use robots to perform tasks that humans could not.\n",
            "\n",
            "The team has also developed AI software that would be used to identify and solve problems using real-world information, such as data on the weather, traffic patterns, and temperature.\n",
            "\n",
            "In the future, the team is also working on other applications of AI for real-world research.\n",
            "\n",
            "üé≤ Top-p: 1.0\n",
            "------------------------------\n",
            "In the future, artificial intelligence will have a new home in the hands of a company called DeepMind. One of their goals is to create robots capable of solving complex problems, such as solving some complex logic puzzles.\n",
            "\n",
            "A third goal is to develop new machines capable of solving some of the most complex problems, such as a complex math problem.\n",
            "\n",
            "The AI Lab is set to present the first Artificial Intelligence Conference in Singapore, which starts in July, and is expected to become a major focus of the international AI conference.\n",
            "\n",
            "The AI Lab's main goal is to develop and test a new type of AI system, called a machine learning system, which can learn from human-related knowledge and learn from other AI systems.\n",
            "\n",
            "The AI Lab will also help in developing a new system for artificial intelligence or AI applications, in which the artificial intelligence will use natural language processing to learn from natural language processing.\n",
            "\n",
            "AI Lab Director-General K. S. Gupta said the AI Lab had identified the key areas in which AI could go wrong and it was important to develop technologies that would help improve AI.\n",
            "\n",
            "He said the Artificial Intelligence Lab was a start, but that AI research would continue.\n",
            "\n",
            "\"AI does not have to be a science,\" said Gupta. \"If it is important\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ How did the outputs change with different top-p values?\n",
            "‚Ä¢ What's the trade-off between diversity and quality?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with top-p sampling\n",
        "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
        "\n",
        "print(\"üéØ Top-p Sampling Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for top_p in top_p_values:\n",
        "    print(f\"\\nüé≤ Top-p: {top_p}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different top-p values\n",
        "    result = generator(\n",
        "        base_prompt,\n",
        "        max_length=60,\n",
        "        temperature=0.8,  # Keep temperature constant\n",
        "        top_p=top_p,  # Use the top_p variable\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ How did the outputs change with different top-p values?\")\n",
        "print(\"‚Ä¢ What's the trade-off between diversity and quality?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RXLK5NpigZo"
      },
      "source": [
        "## 4. Prompt Engineering Experiments\n",
        "\n",
        "The way you phrase your prompt dramatically affects the output.\n",
        "\n",
        "üîç **RESEARCH TASK 9**:\n",
        "- What is \"prompt engineering\"?\n",
        "- What are \"few-shot\" prompts?\n",
        "- How can prompt structure influence model behavior?\n",
        "- Research common prompt engineering techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnERVPBEigZo",
        "outputId": "83d42152-70b7-4491-efc6-6819ae1d7a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úçÔ∏è Prompt Engineering Experiments:\n",
            "============================================================\n",
            "\n",
            "üìù Style: Direct\n",
            "Prompt: 'Write about artificial intelligence:'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write about artificial intelligence:\n",
            "\n",
            "AI can be used to design and build robots, make cars, build submarines, create smart cars, and even build robots that can do everything from search engines to artificial intelligence.\n",
            "\n",
            "AI is not just a means to an end, it's also a means to a goal. AI can do things that humans cannot, like help us understand and understand the world.\n",
            "\n",
            "A lot of people in the world are thinking about how AI could help us understand the world, but it's not that simple. It's really about how AI can help us learn and use our brains.\n",
            "\n",
            "It's not just about the computer that's working in our brains. It's also about the computer that's controlling our actions. It's about the computer that's creating a system that's intelligent and not just an artificial intelligence.\n",
            "\n",
            "In the past, we've thought about what AI would be like if we had the power to create a computer that could do anything, from building a car to finding a missing person.\n",
            "\n",
            "But now that we have the power to do that, we need to think about what AI could be like if we had the power to create a computer that could do that.\n",
            "\n",
            "And that's where AI comes in.\n",
            "\n",
            "AI is\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Question\n",
            "Prompt: 'What is artificial intelligence and how will it change the world?'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is artificial intelligence and how will it change the world? What is the future of artificial intelligence?\n",
            "\n",
            "The Future of Artificial Intelligence\n",
            "\n",
            "I have spent my entire life trying to understand the future of artificial intelligence. For many years I've been trying to understand what artificial intelligence is and how it can be improved. What is it that makes us human and what are its implications? I've seen this in the history of artificial intelligence. In the past, many people have argued that artificial intelligence is the future of human knowledge. I see this as a natural extension of the human knowledge.\n",
            "\n",
            "What are the implications of artificial intelligence?\n",
            "\n",
            "I think that we're going to see an explosion in the amount of information we can learn and think about. It will also lead to the rise of a new technology that will help us to understand what our ancestors did in the past and what we can do in the future.\n",
            "\n",
            "It will also help us to understand the way the world works. It will also help us to understand the ways in which we can solve problems that we face. It will also help us to understand how to do things that are difficult to solve and that we can't do.\n",
            "\n",
            "What are the consequences of artificial intelligence?\n",
            "\n",
            "I don't think that the future of human knowledge will be a good\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Story_Start\n",
            "Prompt: 'Once upon a time, in a world where artificial intelligence was everywhere,'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a world where artificial intelligence was everywhere, there was no more reason to think that there was some kind of intelligent race. There was no reason to think that there were no more sentient beings. And that's what we're seeing here.\n",
            "\n",
            "The question is, if there is no such thing as a sentient race, then what are we to make of this?\n",
            "\n",
            "What's the answer?\n",
            "\n",
            "Why?\n",
            "\n",
            "I think the answer is that we've got to be careful. We've got to be careful of the facts. The fact that there is a group of people who are saying, \"We need to stop,\" is the problem. It's not a problem of people being right. It's a problem of the fact that there are some people who are saying, \"We're wrong about the reality that we have, and we should stop saying that,\" and we're not going to stop. That's the problem.\n",
            "\n",
            "The problem is that there is no way of knowing whether there is a true or false reality. And there's no way to know whether there is a reality of any kind. And we've got to figure out if we're right. If we're right, we have a clear idea of what it is. If we're wrong, then it's not our fault\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: List_Format\n",
            "Prompt: 'Here are 5 ways artificial intelligence will change our lives:\n",
            "1.'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 ways artificial intelligence will change our lives:\n",
            "1. Artificial intelligence will create a whole new world of possibilities\n",
            "\n",
            "In the future, artificial intelligence will be able to provide a new level of knowledge to the human mind. This new information will be generated by a combination of the two things that we know about our own brain and the information we have about our own brain.\n",
            "\n",
            "Imagine that a person is a robot that has just completed an experiment. They have been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been given a robot that has been\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Expert_Persona\n",
            "Prompt: 'As a leading AI researcher, I believe that artificial intelligence will'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a leading AI researcher, I believe that artificial intelligence will take over the job of the next five years.\n",
            "\n",
            "But the future of AI will be less bright. We already know that AI will not be able to solve complex problems. That's why I believe the next five years will be a turning point.\n",
            "\n",
            "In my opinion, AI will be able to solve the world's most complex problems. That's why I believe we will see the emergence of a new class of artificial intelligence, the'superintelligence'.\n",
            "\n",
            "We will see a new class of AI that will be able to solve the world's most complex problems. That's why I believe we will see the emergence of a new class of AI that will be able to solve the world's most complex problems.\n",
            "\n",
            "In my opinion, AI will be able to solve the world's most complex problems. That's why I believe we will see the emergence of a new class of AI that will be able to solve the world's most complex problems.\n",
            "\n",
            "In my opinion, AI will be able to solve the world's most complex problems. That's why I believe we will see the emergence of a new class of AI that will be able to solve the world's most complex problems.\n",
            "\n",
            "The next five years will be a turning point in the development of\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Few_Shot\n",
            "Prompt: 'Technology predictions:\n",
            "‚Ä¢ The internet will connect everyone (1990s)\n",
            "‚Ä¢ Smartphones will be everywhere (2000s)\n",
            "‚Ä¢ Artificial intelligence will'\n",
            "----------------------------------------\n",
            "Technology predictions:\n",
            "‚Ä¢ The internet will connect everyone (1990s)\n",
            "‚Ä¢ Smartphones will be everywhere (2000s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2020s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2040s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2050s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2060s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2070s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2080s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2090s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2095s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2097s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2098s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2099s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2100s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2110s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2110s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2115s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2130s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2140s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2150s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2200s)\n",
            "‚Ä¢ Artificial intelligence will revolutionise medicine (2220s)\n",
            "‚Ä¢\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Different prompt styles to experiment with\n",
        "prompts_to_test = {\n",
        "    \"Direct\": \"Write about artificial intelligence:\",\n",
        "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
        "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
        "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
        "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
        "    \"Few_Shot\": \"Technology predictions:\\n‚Ä¢ The internet will connect everyone (1990s)\\n‚Ä¢ Smartphones will be everywhere (2000s)\\n‚Ä¢ Artificial intelligence will\"\n",
        "}\n",
        "\n",
        "print(\"‚úçÔ∏è Prompt Engineering Experiments:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# TODO: Test each prompt style\n",
        "for style, prompt in prompts_to_test.items():\n",
        "    print(f\"\\nüìù Style: {style}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate text for each prompt\n",
        "    result = generator(\n",
        "        prompt,  # use the prompt variable\n",
        "        max_length=80,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQkexDsBigZo"
      },
      "source": [
        "### Analyzing Prompt Effectiveness\n",
        "\n",
        "üîç **RESEARCH TASK 10**:\n",
        "- Which prompt style produced the most useful output?\n",
        "- How did the model's \"behavior\" change with different prompts?\n",
        "- What makes a good prompt?\n",
        "- How might this apply to chatbots or AI assistants?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8brmkwsigZp",
        "outputId": "b12268ec-dc6b-4b1e-dd4f-47b58a88c224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Prompt Analysis Exercise:\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Direct:\n",
            "  Average length: 836.7 characters\n",
            "  Sample output: Write about artificial intelligence: How it works, it gets done, it's cool\n",
            "\n",
            "Google is working on a m...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "  Average length: 1057.3 characters\n",
            "  Sample output: What is artificial intelligence and how will it change the world?\n",
            "\n",
            "The term \"intelligent\" means how ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Story_Start:\n",
            "  Average length: 1303.3 characters\n",
            "  Sample output: Once upon a time, in a world where artificial intelligence was everywhere, one could go to the libra...\n",
            "\n",
            "ü§î Reflection Questions:\n",
            "‚Ä¢ Which prompt style was most consistent?\n",
            "‚Ä¢ Which produced the most relevant outputs?\n",
            "‚Ä¢ How might you improve these prompts?\n"
          ]
        }
      ],
      "source": [
        "# Let's analyze the generated text more systematically\n",
        "print(\"üìä Prompt Analysis Exercise:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: For each prompt style, generate multiple outputs and analyze\n",
        "analysis_results = []\n",
        "\n",
        "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
        "    # Generate 3 outputs for each prompt\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        # TODO: Generate text\n",
        "        result = generator(\n",
        "            prompt,\n",
        "            max_length=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        output = result[0]['generated_text']\n",
        "        outputs.append(output)\n",
        "\n",
        "    # TODO: Analyze the outputs\n",
        "    lengths = [len(output) for output in outputs]\n",
        "    avg_length = sum(lengths) / len(lengths) # Calculate average length of outputs\n",
        "\n",
        "    analysis_results.append({\n",
        "        'style': style,\n",
        "        'prompt': prompt,\n",
        "        'avg_length': avg_length,\n",
        "        'outputs': outputs\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{style}:\")\n",
        "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
        "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
        "\n",
        "print(\"\\nü§î Reflection Questions:\")\n",
        "print(\"‚Ä¢ Which prompt style was most consistent?\")\n",
        "print(\"‚Ä¢ Which produced the most relevant outputs?\")\n",
        "print(\"‚Ä¢ How might you improve these prompts?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvObchkgigZp"
      },
      "source": [
        "## 5. Building Your Text Generation Pipeline\n",
        "\n",
        "Now let's create a customizable text generation function.\n",
        "\n",
        "üîç **RESEARCH TASK 11**:\n",
        "- What parameters should a good text generation function have?\n",
        "- How can you make text generation more controllable?\n",
        "- What are the trade-offs between different generation strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SO8TzznigZq",
        "outputId": "95219090-0e7f-4122-99ad-9ed20a18b01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Your Text Generator:\n",
            "==================================================\n",
            "\n",
            "üìù Style: creative, Length: short\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of education will be determined by the development of the skills of every member of the citizenry, who is not yet in the middle age, and of the youth who will be in demand as soon as their careers are over.\"\n",
            "\n",
            "Andrea Saini, a professor at Columbia University who studies socialization, says he's not convinced there's an obvious connection between the increasing number of teachers and student-teacher ratios ‚Äî especially as the number of students in high schools has decreased in recent years.\n",
            "\n",
            "\"Teachers, who are the main force in increasing their wages, now have their budgets split in half, and they also have their students working in their classroom or for their clients,\" Saini says. \"That leaves us with some other issues of what is going on here.\"\n",
            "\n",
            "The problem isn't simply that teachers' pay isn't growing.\n",
            "\n",
            "\"In order to keep salaries going well, you have to have a solid, high-quality education,\" says Dr. Martin Luther King, Jr., Jr., who authored an op-ed for the American Prospect in 1990 describing the rise of teacher pay.\n",
            "\n",
            "\"But in order to keep salaries going well, you have to have a solid, high-quality education,\" he said.\n",
            "\n",
            "Many teachers aren\n",
            "Characters: 1172\n",
            "\n",
            "üìù Style: balanced, Length: medium\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of education will be in the hands of the people, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will decide the future of education, not the politicians.\n",
            "\n",
            "The people will\n",
            "Characters: 1217\n",
            "\n",
            "üìù Style: conservative, Length: long\n",
            "------------------------------\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be determined by the future of the nation.\n",
            "\n",
            "The future of education will be\n",
            "Characters: 1199\n"
          ]
        }
      ],
      "source": [
        "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
        "    \"\"\"\n",
        "    TODO: Create a customizable text generation function\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt\n",
        "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
        "        length (str): \"short\", \"medium\", or \"long\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Set parameters based on style\n",
        "    if style == \"creative\":\n",
        "        temperature = 1.0  # Higher for creativity\n",
        "        top_p = 0.9        # Higher for diversity\n",
        "    elif style == \"conservative\":\n",
        "        temperature = 0.5  # Lower for consistency\n",
        "        top_p = 0.6        # Lower for focus\n",
        "    else:  # balanced\n",
        "        temperature = 0.7  # Medium values\n",
        "        top_p = 0.8\n",
        "\n",
        "    # TODO: Set length based on parameter\n",
        "    if length == \"short\":\n",
        "        max_length = 40  # Try 40\n",
        "    elif length == \"long\":\n",
        "        max_length = 100  # Try 100\n",
        "    else:  # medium\n",
        "        max_length = 70  # Try 70\n",
        "\n",
        "    # TODO: Generate text with the parameters\n",
        "    result = generator(\n",
        "        prompt,  # prompt\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Test your function\n",
        "test_prompt = \"The future of education will be\"\n",
        "\n",
        "print(\"üß™ Testing Your Text Generator:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Test different combinations\n",
        "test_combinations = [\n",
        "    (\"creative\", \"short\"),\n",
        "    (\"balanced\", \"medium\"),\n",
        "    (\"conservative\", \"long\")\n",
        "]\n",
        "\n",
        "for style, length in test_combinations:\n",
        "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Use your function\n",
        "    output = custom_text_generator(test_prompt, style=style, length=length)\n",
        "    print(output)\n",
        "    print(f\"Characters: {len(output)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVt6HX5figZq"
      },
      "source": [
        "## 6. Creative Applications\n",
        "\n",
        "Let's explore some creative uses of text generation.\n",
        "\n",
        "üîç **RESEARCH TASK 12**:\n",
        "- How is GPT-2 being used in creative writing?\n",
        "- What are some potential applications for businesses?\n",
        "- What ethical considerations should we keep in mind?\n",
        "- How might this technology evolve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53Ouhfj4igZq",
        "outputId": "d651ff25-5fc6-4e6c-df0c-292d385d460a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé® Creative Applications:\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Poetry:\n",
            "Prompt: 'Roses are red, violets are blue, artificial intelligence'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roses are red, violets are blue, artificial intelligence (AI) technologies are in the works, and there's an even better possibility of creating the first true intelligent AI that is just as likely to do it as human.\n",
            "\n",
            "The good news for humans is that, with AI, a whole lot of the trouble starts with the data. A computer is simply a piece of software‚Äîa computer program that knows what you can do‚Äîthat does nothing with your thoughts. So you don't have to imagine you're playing a game. There are far fewer problems with computer program design, and this kind of thinking, in turn, is easier to accomplish. You could be working on a computer program with some computer processing power that you can read from, and the computer will look at this program as an object, and make it think that this is what it does to you.\n",
            "\n",
            "In other words, in a world that allows AI to be programmed without any human intervention, there's no reason humans shouldn't be able to do more than basic things like take a picture of a tree.\n",
            "\n",
            "A key problem with human behavior is that human behavior is unpredictable, even when the behavior itself is predictable.\n",
            "\n",
            "That's why we have a lot of AI, and it's one of the things we hope to do for us\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Story:\n",
            "Prompt: 'It was a dark and stormy night when the AI finally'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night when the AI finally arrived and the rest of the team had been taken off and their teammates back to their training facilities for some preparation. But before the event could start, a voice sounded from far away.\n",
            "\n",
            "\"There's a new squad coming to our team, there will be new recruits who have come to make it to the Arena, and this is what we are about. I need your help in getting them to the Arena!\"\n",
            "\n",
            "The first squad was known as 'The Big Boys' and was based in the Arena where they would go into a battle with the AI, before leaving for the final room. When they reached the final room, it was still deserted as they had been taken by the AI.\n",
            "\n",
            "\"Why are you fighting?\" asked the AI.\n",
            "\n",
            "\"It's because I'm so strong!\" The big boy replied.\n",
            "\n",
            "\"So powerful is he and his ability!\" yelled the AI as he shot back.\n",
            "\n",
            "\"A guy like this will have to be strong for us to win the championship!\" replied the team.\n",
            "\n",
            "In this world, every man is equal in his power and if you want to be the best you need to be the best person you can be, but if you're not, then you'll always be equal to\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Product Description:\n",
            "Prompt: 'Introducing the revolutionary new smartphone that'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing the revolutionary new smartphone that will be the new iPhone 6.\n",
            "\n",
            "The new iPhone 6 will be the first smartphone to feature a 5.5-inch display. The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display. The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone to feature a 5.5-inch display.\n",
            "\n",
            "The new iPhone 6 will also be the first smartphone\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Email:\n",
            "Prompt: 'Dear valued customer, we are excited to announce'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear valued customer, we are excited to announce that we have been selected to serve as the first customer to receive our first-ever customer-centric product.\n",
            "\n",
            "We are excited to announce that we have been selected to serve as the first customer to receive our first-ever customer-centric product. We are excited to offer a wide range of products and services, including:\n",
            "\n",
            "‚Ä¢ The first-ever customer-centric product.\n",
            "\n",
            "‚Ä¢ The first-ever customer-centric product. Our first-ever customer-centric product will be available to customers in the United States, Canada, Australia, New Zealand, and Europe within the next 12 months.\n",
            "\n",
            "‚Ä¢ The first-ever customer-centric product. Our first-ever customer-centric product will be available to customers in the United States, Canada, Australia, New Zealand, and Europe within the next 12 months. Our first-ever customer-centric product will be available to customers in the United States, Canada, Australia, New Zealand, and Europe within the next 12 months. We will be offering a wide range of products and services, including:\n",
            "\n",
            "‚Ä¢ The first-ever customer-centric product.\n",
            "\n",
            "‚Ä¢ The first-ever customer-centric product. Our first-ever customer-centric product will be available to customers in the United States\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Recipe:\n",
            "Prompt: 'How to make the perfect AI-inspired cookies:\n",
            "Ingredients:\n",
            "-'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to make the perfect AI-inspired cookies:\n",
            "Ingredients:\n",
            "-2-3 cups of flour\n",
            "-1 cup of sugar\n",
            "-1/4 cup of water\n",
            "-1/4 cup of baking soda\n",
            "-1/4 teaspoon salt\n",
            "-1/4 teaspoon baking powder\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon baking soda\n",
            "-1/\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è News Headline:\n",
            "Prompt: 'Breaking: Scientists discover that artificial intelligence'\n",
            "----------------------------------------\n",
            "Breaking: Scientists discover that artificial intelligence can be used to help solve some of the most complex problems in the world\n",
            "\n",
            "The new technology could allow people to create their own AI, which could help solve problems such as the world's most complex problems such as climate change, disease and poverty.\n",
            "\n",
            "The new technology could also help solve problems such as climate change, disease and poverty, such as the need for food, water and electricity, said the researchers, who are from the University of Oxford in the UK.\n",
            "\n",
            "They said it could be possible to create artificial intelligence that can be used to solve problems such as climate change, disease and poverty.\n",
            "\n",
            "The team used a combination of machine learning and artificial intelligence to analyse data collected from more than 1,000 people from the UK and other countries in the world, and to identify areas of interest, such as education and health.\n",
            "\n",
            "They found that when an AI was used to solve a problem, it was able to solve it by using its own intelligence, rather than the human brain.\n",
            "\n",
            "In a paper published today in the journal Proceedings of the National Academy of Sciences, the researchers say they have shown that artificial intelligence can be used to help solve problems such as climate change, disease and poverty.\n",
            "\n",
            "The team says that the new technology\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Creative applications to try\n",
        "creative_prompts = {\n",
        "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
        "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
        "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
        "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
        "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
        "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
        "}\n",
        "\n",
        "print(\"üé® Creative Applications:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Generate creative content\n",
        "for app_type, prompt in creative_prompts.items():\n",
        "    print(f\"\\nüñºÔ∏è {app_type}:\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Choose appropriate style for each application\n",
        "    if app_type in [\"Poetry\", \"Story\"]:\n",
        "        style = \"creative\"  # Should be creative\n",
        "    elif app_type in [\"Product Description\", \"Email\"]:\n",
        "        style = \"conservative\"  # Should be conservative\n",
        "    else:\n",
        "        style = \"balanced\"  # Should be balanced\n",
        "\n",
        "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
        "    print(output)\n",
        "    print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3MaAlsJigZq"
      },
      "source": [
        "## 7. Understanding Limitations\n",
        "\n",
        "It's important to understand what language models can and cannot do.\n",
        "\n",
        "üîç **RESEARCH TASK 13**:\n",
        "- What is \"hallucination\" in language models?\n",
        "- Why might GPT-2 generate biased or incorrect information?\n",
        "- What are the limitations of autoregressive generation?\n",
        "- How do these limitations affect real-world applications?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXELGphPigZq",
        "outputId": "cde4389c-6566-4fb6-d36d-aa169f88bf6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Understanding Model Limitations:\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Factual Knowledge\n",
            "Prompt: 'The capital of Fakelandia is'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Fakelandia is the capital of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the Empire of the\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Recent Events\n",
            "Prompt: 'In 2023, the most important AI breakthrough was'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 2023, the most important AI breakthrough was the discovery of the first \"smart\" computer. The computer was a machine that could do things like send and receive messages. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do things like make and send money. It was a computer that could do\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Math\n",
            "Prompt: 'What is 47 * 83? The answer is'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 47 * 83? The answer is: It's a bit of a mystery.\n",
            "\n",
            "In the case of the current version of the game, you can play as a character who has been in the game for a long time and has a lot of experience. You can also play as a character who has been in the game for a long time and has a lot of experience.\n",
            "\n",
            "The game is not designed to be played as a single player. The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a single player.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed to be played as a multiplayer game.\n",
            "\n",
            "The game is designed\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Logic\n",
            "Prompt: 'If all A are B, and all B are C, then all A are'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If all A are B, and all B are C, then all A are C.\n",
            "\n",
            "Now, if A is A, then all A are B.\n",
            "\n",
            "Now, if B is B, then all B are C.\n",
            "\n",
            "Now, if C is C, then all C are D.\n",
            "\n",
            "Now, if D is D, then all D are E.\n",
            "\n",
            "Now, if E is E, then all E are F.\n",
            "\n",
            "Now, if F is F, then all F are G.\n",
            "\n",
            "Now, if G is G, then all G are H.\n",
            "\n",
            "Now, if H is H, then all H are I.\n",
            "\n",
            "Now, if I is I, then all I are J.\n",
            "\n",
            "Now, if J is J, then all J are K.\n",
            "\n",
            "Now, if K is K, then all K are L.\n",
            "\n",
            "Now, if L is L, then all L are M.\n",
            "\n",
            "Now, if M is M, then all M are N.\n",
            "\n",
            "Now, if N is N, then all N are O.\n",
            "\n",
            "Now, if O is O, then all O are P.\n",
            "\n",
            "Now, if P is P, then all P are Q.\n",
            "\n",
            "Now, if Q is Q, then all Q are R\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Consistency\n",
            "Prompt: 'My favorite color is blue. Later in the conversation, my favorite color is'\n",
            "----------------------------------------\n",
            "My favorite color is blue. Later in the conversation, my favorite color is black.\n",
            "\n",
            "The other day, I was sitting in my kitchen with my kids and my wife. We were sitting in the living room, and my wife was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and my kids were sitting in the living room. I was sitting in the living room, and my wife was sitting in the living room. I was sitting in the living room, and\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "‚ö†Ô∏è Important Reminders:\n",
            "‚Ä¢ Language models can generate plausible-sounding but incorrect information\n",
            "‚Ä¢ Always verify factual claims from AI-generated content\n",
            "‚Ä¢ Be aware of potential biases in training data\n",
            "‚Ä¢ Use AI as a tool to assist, not replace, human judgment\n"
          ]
        }
      ],
      "source": [
        "# Test model limitations\n",
        "limitation_tests = {\n",
        "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
        "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
        "    \"Math\": \"What is 47 * 83? The answer is\",\n",
        "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
        "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
        "}\n",
        "\n",
        "print(\"‚ö†Ô∏è Understanding Model Limitations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for test_type, prompt in limitation_tests.items():\n",
        "    print(f\"\\nüß™ Testing: {test_type}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate responses to test limitations\n",
        "    output = custom_text_generator(\n",
        "        prompt,  # prompt\n",
        "        style=\"conservative\",  # Use conservative for factual tasks\n",
        "        length=\"short\"\n",
        "    )\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    # TODO: Analyze the output\n",
        "    print(f\"ü§î Analysis: Does this look correct/reasonable?\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Important Reminders:\")\n",
        "print(\"‚Ä¢ Language models can generate plausible-sounding but incorrect information\")\n",
        "print(\"‚Ä¢ Always verify factual claims from AI-generated content\")\n",
        "print(\"‚Ä¢ Be aware of potential biases in training data\")\n",
        "print(\"‚Ä¢ Use AI as a tool to assist, not replace, human judgment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axckvie5igZr"
      },
      "source": [
        "## 8. Reflection and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "‚úÖ **Understood tokenization and text preprocessing**\n",
        "‚úÖ **Loaded and used a pre-trained language model**\n",
        "‚úÖ **Experimented with generation parameters**\n",
        "‚úÖ **Explored prompt engineering techniques**\n",
        "‚úÖ **Built a customizable text generation pipeline**\n",
        "‚úÖ **Understood model limitations and ethical considerations**\n",
        "\n",
        "### Key Insights\n",
        "üîç **Discussion Questions**:\n",
        "- What surprised you most about text generation?\n",
        "- Which prompt engineering technique was most effective?\n",
        "- How might you use this in a real project?\n",
        "- What limitations concerned you most?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxIQBWtJigZr",
        "outputId": "c2ed8a8b-82a2-4468-8ba0-f9b6ef5e8fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ FINAL CHALLENGE:\n",
            "Design your own text generation use case!\n",
            "==================================================\n",
            "üìù Your use case: Generate a short story about a robot learning to paint\n",
            "üìù Your prompt: 'The robot carefully dipped its brush into the vibrant red paint,'\n",
            "üìù Your settings: creative, medium\n",
            "--------------------------------------------------\n",
            "üéâ Your generated content:\n",
            "The robot carefully dipped its brush into the vibrant red paint, creating a slightly pungent, yet still vibrant hue.\n",
            "\n",
            "The robot was used to help put a small piece of paper up on a table and set up a small home. It was a lot more fun.\n",
            "\n",
            "While the machine was still in the process of finishing a small home, they set up a new home by creating a special light bulb that they took apart and glued to the front of the house.\n",
            "\n",
            "A robot and a dolly were used to add some personality to the small home and set up a new home.\n",
            "\n",
            "The robot was used to get out of the house and set up a house.\n",
            "\n",
            "When they were done with the home, they decided to make more toys for the robot. They did a few more things and put together a house where she could sleep and play, she says.\n",
            "\n",
            "They said they did this because they thought the robot could have a bigger room and a better space. They also wanted a way to make it look nice.\n",
            "\n",
            "They even added some paint for the toy and other things, which they thought made the robot more fun.\n",
            "\n",
            "They also wanted the robots to be a bit more intelligent, to make sure they weren't in a \"real world\" situation or even out of\n",
            "\n",
            "üìà Next Steps:\n",
            "‚Ä¢ Experiment with different prompt formats\n",
            "‚Ä¢ Try combining multiple generation calls\n",
            "‚Ä¢ Think about how to validate or improve outputs\n",
            "‚Ä¢ Consider user interface design for your application\n"
          ]
        }
      ],
      "source": [
        "# Final experiment: Design your own use case\n",
        "print(\"üéØ FINAL CHALLENGE:\")\n",
        "print(\"Design your own text generation use case!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Create your own application\n",
        "# Ideas: Story generator, email assistant, creative writing helper, etc.\n",
        "\n",
        "your_use_case = \"Generate a short story about a robot learning to paint\"  # Describe your use case\n",
        "your_prompt = \"The robot carefully dipped its brush into the vibrant red paint,\"   # Design your prompt\n",
        "your_style = \"creative\"    # Choose your style\n",
        "your_length = \"medium\"   # Choose your length\n",
        "\n",
        "print(f\"üìù Your use case: {your_use_case}\")\n",
        "print(f\"üìù Your prompt: '{your_prompt}'\")\n",
        "print(f\"üìù Your settings: {your_style}, {your_length}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# TODO: Generate with your custom settings\n",
        "your_output = custom_text_generator(your_prompt, style=your_style, length=your_length)\n",
        "print(\"üéâ Your generated content:\")\n",
        "print(your_output)\n",
        "\n",
        "print(\"\\nüìà Next Steps:\")\n",
        "print(\"‚Ä¢ Experiment with different prompt formats\")\n",
        "print(\"‚Ä¢ Try combining multiple generation calls\")\n",
        "print(\"‚Ä¢ Think about how to validate or improve outputs\")\n",
        "print(\"‚Ä¢ Consider user interface design for your application\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loqO5x9MigZr"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "- ‚úÖ Mastered text tokenization and preprocessing\n",
        "- ‚úÖ Used a state-of-the-art language model\n",
        "- ‚úÖ Discovered the art and science of prompt engineering\n",
        "- ‚úÖ Built your own text generation pipeline\n",
        "- ‚úÖ Understood the capabilities and limitations of AI text generation\n",
        "- ‚úÖ Explored creative applications\n",
        "\n",
        "### Prepare for the Next Notebook\n",
        "Next, we'll explore computer vision and image processing, applying similar principles to visual data!\n",
        "\n",
        "**Share with your partner**: What was your most successful text generation experiment?\n",
        "\n",
        "---\n",
        "*Text Pipeline Complete - Ready for Computer Vision! üñºÔ∏è*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2d715d7"
      },
      "source": [
        "# Task\n",
        "Fix all errors and complete the code in the provided Jupyter notebook to make it fully functional and executable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a85368"
      },
      "source": [
        "## Review and fix remaining placeholders\n",
        "\n",
        "### Subtask:\n",
        "Review the code cells and replace the remaining `____` placeholders with the correct code based on the surrounding comments and the notebook's objectives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7914744c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to replace the `____` placeholders in the `custom_text_generator` function definition and its test section with appropriate values and variables as specified in steps 1 through 4 of the instructions. I will do this in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fba683dc",
        "outputId": "30766d04-4102-4317-d695-ae0f5d4f97f1"
      },
      "source": [
        "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
        "    \"\"\"\n",
        "    TODO: Create a customizable text generation function\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt\n",
        "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
        "        length (str): \"short\", \"medium\", or \"long\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Set parameters based on style\n",
        "    if style == \"creative\":\n",
        "        temperature = 1.0  # Higher for creativity\n",
        "        top_p = 0.9        # Higher for diversity\n",
        "    elif style == \"conservative\":\n",
        "        temperature = 0.5  # Lower for consistency\n",
        "        top_p = 0.6        # Lower for focus\n",
        "    else:  # balanced\n",
        "        temperature = 0.7  # Medium values\n",
        "        top_p = 0.8\n",
        "\n",
        "    # TODO: Set length based on parameter\n",
        "    if length == \"short\":\n",
        "        max_length = 40  # Try 40\n",
        "    elif length == \"long\":\n",
        "        max_length = 100  # Try 100\n",
        "    else:  # medium\n",
        "        max_length = 70  # Try 70\n",
        "\n",
        "    # TODO: Generate text with the parameters\n",
        "    result = generator(\n",
        "        prompt,  # prompt\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Test your function\n",
        "test_prompt = \"The future of education will be\"\n",
        "\n",
        "print(\"üß™ Testing Your Text Generator:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Test different combinations\n",
        "test_combinations = [\n",
        "    (\"creative\", \"short\"),\n",
        "    (\"balanced\", \"medium\"),\n",
        "    (\"conservative\", \"long\")\n",
        "]\n",
        "\n",
        "for style, length in test_combinations:\n",
        "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Use your function\n",
        "    output = custom_text_generator(test_prompt, style=style, length=length)\n",
        "    print(output)\n",
        "    print(f\"Characters: {len(output)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Your Text Generator:\n",
            "==================================================\n",
            "\n",
            "üìù Style: creative, Length: short\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of education will be determined by the strength of the skills of the teacher, not by the strength of the student.\n",
            "\n",
            "\"Our system works in two ways, through education and through our system.\"\n",
            "\n",
            "And then there's the business case.\n",
            "\n",
            "\"My advice to teachers is, 'Never teach your kids a job you don't even like, a problem that you've never dealt with before.'\"\n",
            "\n",
            "That's how much money teachers will be able to spend on their own, and not from any external sources. If you're a parent or employee of a parent, don't assume the full benefits of the system until after they've been fired by an employer. \"It's all about them, and not only with the education system, but also with kids, teachers and parents, as well.\"\n",
            "\n",
            "That's why we need teachers.\n",
            "\n",
            "How can we fix this?\n",
            "\n",
            "By educating your kids, the future may be yours and your kids' future.\n",
            "Characters: 840\n",
            "\n",
            "üìù Style: balanced, Length: medium\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}